{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6563a0b",
   "metadata": {},
   "source": [
    "## [Main Playlist we study from](<(https://www.youtube.com/playlist?list=PLb49csYFtO2HAdNGChGzohFJGnJnXBOqd)>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bff1ba8",
   "metadata": {},
   "source": [
    "#### What are images..?\n",
    "\n",
    "1. _Images are Numpy Arrays made by pixels_\n",
    "2. _Image Shape : (Height, Width, Number of Channels)_\n",
    "3. _Pixel values range from 0 to 255_\n",
    "   1. _In Binary Image -> Pixel Values are 0(OFF) and 1(ON) OR 255_\n",
    "   2. _In 16-bit Images , pixel values ranges from 0 to 65535_\n",
    "4. \\*2 Popular types of images : (Gray-Scale & RGB)\n",
    "   1. RGB has 3 Channels (3 Gray-Scale images mixed together) -> 3D Array OR 3 of 2D Arrays\n",
    "   2. Gray-Scale images has only 1 channed -> 2D Array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0becc774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220, 352, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Read an image\n",
    "img = cv2.imread('../Images/cat.jpg')\n",
    "\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3dd43668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uint8\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(img.dtype)\n",
    "print(type(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "096a28d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write(Save) an Image\n",
    "cv2.imwrite('cat.jpg', img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize an Image\n",
    "cv2.imshow('CAT', img)\n",
    "cv2.waitKey(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab226a1",
   "metadata": {},
   "source": [
    "#### Deal with Videos\n",
    "\n",
    "_video : series of images (frames)_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'cv2.VideoCapture'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "< cv2.VideoCapture 0000021DAE31B8F0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Video\n",
    "video = cv2.VideoCapture('video.mp4')\n",
    "\n",
    "print(type(video))\n",
    "video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08928250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visaulize Video\n",
    "ret, frame = video.read()\n",
    "while ret:\n",
    "    cv2.imshow('frame', frame)\n",
    "    key = cv2.waitKey(40) & 0xFF\n",
    "    # 40 = Amount of Time you should wait between different frames\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "    ret, frame = video.read()\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977c5876",
   "metadata": {},
   "source": [
    "#### Webcam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a2a30cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "< cv2.VideoCapture 0000021DAE31BF70>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read a webcam\n",
    "webcam = cv2.VideoCapture(0)\n",
    "\n",
    "webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6f51011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a webcam\n",
    "while True:\n",
    "    ret, frame = webcam.read()\n",
    "    cv2.imshow('Frame', frame)\n",
    "\n",
    "    key = cv2.waitKey(40) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "webcam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c469ba",
   "metadata": {},
   "source": [
    "#### Basic Image Operations\n",
    "\n",
    "1. Resizing : Scale up/down the image.\n",
    "2. Cropping : Focus on important regions and discard the other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d0e5e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resizing\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread('../house.png')\n",
    "\n",
    "cv2.imshow('Original Image', img)\n",
    "\n",
    "img_resized = cv2.resize(img, dsize=(540, 800))\n",
    "\n",
    "cv2.imshow('Resized Image', img_resized)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc48ce14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(213, 326, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cropping\n",
    "\n",
    "print(img.shape)\n",
    "cv2.imshow('Original Image', img)\n",
    "\n",
    "# As image is a numpy array -> you can use indices to choose the rows and columns\n",
    "\n",
    "cropped_img = img[50:200, 200:300]\n",
    "cv2.imshow('Cropped Image', cropped_img)\n",
    "\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cebf844",
   "metadata": {},
   "source": [
    "#### Colorspaces\n",
    "\n",
    "[OpenCV Colorspaces](https://opencv.org/blog/color-spaces-in-opencv/#:~:text=While%20the%20RGB%20%28Red%2C%20Green%2C%20Blue%29%20is%20the,offering%20unique%20advantages%20for%20different%20image%20processing%20tasks.)\n",
    "\n",
    "1. BGR = Blue, Green, Red -> cv2 read a colored image ar BGR by default\n",
    "2. RGB = Red, Green, Blue\n",
    "3. Gray-Scale = Shades of Gray\n",
    "4. HSV = Hue, Saturation, Value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f2fe37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread('../bird.png')\n",
    "cv2.imshow('BGR BIRD', img)\n",
    "\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "cv2.imshow('RGB BIRD', img_rgb)\n",
    "\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "# Only 1 Channel , We lose alot of information\n",
    "cv2.imshow('Gray BIRD', img_gray)\n",
    "\n",
    "img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "cv2.imshow('HSV BIRD', img_hsv)\n",
    "\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a221516f",
   "metadata": {},
   "source": [
    "#### Image Blurring : Smooth Image & Remove noise\n",
    "\n",
    "[Theory](https://docs.opencv.org/4.x//dc/dd3/tutorial_gausian_median_blur_bilateral_filter.html)\n",
    "\n",
    "[Code Example](<(https://docs.opencv.org/4.x/d4/d13/tutorial_py_filtering.html)>)\n",
    "\n",
    "1. Normalized Box Filter\n",
    "2. Gaussian Filter\n",
    "3. Median Filter\n",
    "4. Bilateral Filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c84846a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread('../person.png')\n",
    "\n",
    "cv2.imshow('Person', img)\n",
    "\n",
    "# If we increase the kernel size -> Blurring increase but we can lose some information\n",
    "kernel_size = 10  # Always square matrix (Even or Odd)\n",
    "blurred_img = cv2.blur(img, ksize=(kernel_size, kernel_size))\n",
    "cv2.imshow('Blurred Person', blurred_img)\n",
    "\n",
    "# Gaussian Blur : Kernel must be Odd\n",
    "gauss_blur_img = cv2.GaussianBlur(img, ksize=(15, 15), sigmaX=3.0)\n",
    "cv2.imshow('Gausssian Blurred Person', gauss_blur_img)\n",
    "\n",
    "\n",
    "# Median Blur : Kernel must be Odd\n",
    "median_blur_img = cv2.medianBlur(img, 13)\n",
    "cv2.imshow('Median Blurred Person', median_blur_img)\n",
    "\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f50162e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread('../noise.png')\n",
    "\n",
    "cv2.imshow('Noised Image', img)\n",
    "\n",
    "blurred_img = cv2.blur(img, ksize=(6, 5))\n",
    "cv2.imshow('Blurred Person', blurred_img)\n",
    "\n",
    "gauss_blur_img = cv2.GaussianBlur(img, ksize=(15, 15), sigmaX=3.0)\n",
    "cv2.imshow('Gausssian Blurred Person', gauss_blur_img)\n",
    "\n",
    "median_blur_img = cv2.medianBlur(img, 13)\n",
    "cv2.imshow('Median Blurred Person', median_blur_img)\n",
    "\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce03083",
   "metadata": {},
   "source": [
    "#### Thresholding\n",
    "\n",
    "[OpenCV Thresholding](<(https://docs.opencv.org/3.4/d7/d4d/tutorial_py_thresholding.html)>)\n",
    "\n",
    "##### THresholding : If the pixel value is smaller than the threshold, it is set to 0, otherwise it is set to a maximum value.\n",
    "\n",
    "_The function cv.threshold(..,..,..,..) is used to apply the thresholding_\n",
    "\n",
    "1. The first argument is the source image, which should be a grayscale image.\n",
    "2. The second argument is the threshold value which is used to classify the pixel values.\n",
    "3. The third argument is the maximum value which is assigned to pixel values exceeding the threshold.\n",
    "4. OpenCV provides different types of thresholding which is given by the fourth parameter of the function.\n",
    "\n",
    "##### Simple Global Thresholding\n",
    "\n",
    "1. cv.THRESH_BINARY\n",
    "2. cv.THRESH_BINARY_INV\n",
    "3. cv.THRESH_TRUNC\n",
    "4. cv.THRESH_TOZERO\n",
    "5. cv.THRESH_TOZERO_INV\n",
    "\n",
    "##### Adaptive Thresholding\n",
    "\n",
    "1. cv.ADAPTIVE_THRESH_MEAN_C: The threshold value is the mean of the neighbourhood area minus the constant C.\n",
    "2. cv.ADAPTIVE_THRESH_GAUSSIAN_C: The threshold value is a gaussian-weighted sum of the neighbourhood values minus the constant C.\n",
    "\n",
    "_The blockSize determines the size of the neighbourhood area and C is a constant that is subtracted from the mean or weighted sum of the neighbourhood pixels._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2286638e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "img = cv2.imread('../bear.png')\n",
    "cv2.imshow('BGR image', img)\n",
    "\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow('Gray-Scale image', img_gray)\n",
    "\n",
    "# The method returns two outputs. The first is the threshold that was used and the second output is the thresholded image.\n",
    "ret, threshold = cv2.threshold(\n",
    "    src=img_gray, thresh=80, maxval=255, type=cv2.THRESH_BINARY)\n",
    "cv2.imshow('THRESH-BINARY image', threshold)\n",
    "\n",
    "threshold = cv2.blur(threshold, (7, 7))\n",
    "ret, threshold = cv2.threshold(\n",
    "    src=img_gray, thresh=80, maxval=255, type=cv2.THRESH_BINARY)\n",
    "cv2.imshow('BLUR-THRESH-BINARY image', threshold)\n",
    "\n",
    "\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3b01fc",
   "metadata": {},
   "source": [
    "_Adaptive Thresholding : the algorithm determines the threshold for a pixel based on a small region around it. So we get different thresholds for different regions of the same image which gives better results for images with varying illumination._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405fa782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread('../paper.png')\n",
    "cv2.imshow('BGR image', img)\n",
    "\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow('Gray-Scale image', img_gray)\n",
    "\n",
    "ret, threshold = cv2.threshold(\n",
    "    src=img_gray, thresh=80, maxval=255, type=cv2.THRESH_BINARY)\n",
    "cv2.imshow('THRESH-BINARY image', threshold)\n",
    "\n",
    "adaptive_threshold = cv2.adaptiveThreshold(\n",
    "    img_gray, 250, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 21, 30)\n",
    "cv2.imshow('ADAPTIVE-THRESH-GAUSS image', adaptive_threshold)\n",
    "\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df929209",
   "metadata": {},
   "source": [
    "#### Edge Detection : identify sharp changes in brightness (boundaries between different regions) that typically signify object boundaries, edges, lines, or textures.\n",
    "\n",
    "1. Sobel : discrete differentiation operator that computes an approximation of the gradient of the image intensity function. It emphasizes edge detection in both horizontal and vertical directions by combining Gaussian smoothing and differentiation, making it less sensitive to noise.\n",
    "\n",
    "2. Laplacian : second-order derivative operator that highlights regions of rapid intensity change, effective in edge detection. Unlike Sobel, which is directional, the Laplacian is non-directional and detects edges in all directions.\n",
    "\n",
    "3. Canny : It combines gradient-based edge detection with advanced logic to ensure that the detected edges are thin, connected, and free from noise. Canny Edge Detection is one of the most popular edge-detection methods in use today because it is so robust and flexible. The algorithm itself follows a three-stage process for extracting edges from an image. Add to it image blurring, a necessary preprocessing step to reduce noise. This makes it a four-stage process, which includes:\n",
    "\n",
    "   1. Noise Reduction\n",
    "   2. Calculating the Intensity Gradient of the Image\n",
    "   3. Suppression of False Edges\n",
    "   4. Hysteresis Thresholding\n",
    "\n",
    "_Edge in an image represents a boundary where there is a significant change in intensity or color. Edge detection is crucial for understanding the structure and features within an image, aiding in tasks like object recognition, segmentation, and tracking. Edges are typically detected by identifying areas with high intensity gradients, which can be achieved using various operators that compute derivatives of the image intensity function._\n",
    "\n",
    "_Note:- To detect edges in color images, we typically convert the image to a color space where the intensity and color information are more easily separable or we compute gradients in each channel separately (grayscale)_\n",
    "\n",
    "######\n",
    "\n",
    "=============================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d78fd36",
   "metadata": {},
   "source": [
    "_Sobel Edge-Detection_ -> cv2.Sobel(src, ddepth, dx, dy, ksize=3)\n",
    "\n",
    "-\n",
    "\n",
    "1. src: Input image (should be grayscale for edge detection).\n",
    "2. dst (C++ only): Output image where the result is stored.\n",
    "3. ddepth: Desired depth of the output image (e.g., CV_64F allows negative gradients).\n",
    "4. dx: Order of the derivative in the x-direction (set to 1 to detect horizontal changes).\n",
    "5. dy: Order of the derivative in the y-direction (set to 1 to detect vertical changes).\n",
    "6. ksize: Size of the extended Sobel kernel (must be odd: 1, 3, 5, 7; use 1 for Scharr operator).\n",
    "\n",
    "-\n",
    "\n",
    "<img src='../sobel.png'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bbf5d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread('../tiger.png')\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect Horizontal Edges along X-direction\n",
    "sobelx = cv2.Sobel(src=img_gray, ddepth=cv2.CV_64F, dx=1, dy=0, ksize=3)\n",
    "# Detect Vertical Edges along Y-direction\n",
    "sobely = cv2.Sobel(src=img_gray, ddepth=cv2.CV_64F, dx=0, dy=1, ksize=3)\n",
    "\n",
    "# Combine the horizontal and vertical components -> Overall Edge Strength\n",
    "gradient_magnitude = cv2.magnitude(sobelx, sobely)\n",
    "# Convert it to uint8 (8-bit Unsigned Integer)\n",
    "gradient_magnitude = cv2.convertScaleAbs(gradient_magnitude)\n",
    "\n",
    "cv2.imshow('Sobel Edge-Detection', gradient_magnitude)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac0e706",
   "metadata": {},
   "source": [
    "_Laplacian Edge-Detection_ -> cv2.Sobel(src, ddepth, ksize=3)\n",
    "\n",
    "-\n",
    "\n",
    "1. src: Input image (usually in grayscale).\n",
    "2. dst (C++ only): Output image to store the Laplacian result.\n",
    "3. ddepth: Desired depth of the output image (e.g., CV_64F to capture negative values).\n",
    "4. ksize: Size of the Laplacian kernel (must be odd and positive; typically 1, 3, 5, or 7). Use ksize=1 to apply a 3×3 kernel without scaling.\n",
    "\n",
    "-\n",
    "\n",
    "<img src='../laplacian.png'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bebc54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laplacian = cv2.Laplacian(src=img_gray, ddepth=cv2.CV_64F, ksize=3)\n",
    "\n",
    "# Since the Laplacian output includes negative values (indicating direction of change),\n",
    "# it’s converted to an absolute 8-bit format using convertScaleAbs to make it visually displayable.\n",
    "laplacian = cv2.convertScaleAbs(laplacian)\n",
    "\n",
    "cv2.imshow('Laplacian Edge-Detection', laplacian)\n",
    "\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a392c1",
   "metadata": {},
   "source": [
    "_Canny Edge-Detection_ -> cv2.Canny(image, threshold1, threshold2)\n",
    "\n",
    "-\n",
    "\n",
    "1. image: Input image (must be in grayscale).\n",
    "2. edges (C++ only): Output image where edges will be marked as white (255) on a black background.\n",
    "3. threshold1: Lower boundary for the hysteresis thresholding.\n",
    "4. threshold2: Upper boundary for the hysteresis thresholding.\n",
    "\n",
    "-\n",
    "\n",
    "<img src='../noise-reduction.png'>\n",
    "<img src='../intensity-gradient.png'>\n",
    "<img src='../suppression.png'>\n",
    "<img src='../hysteresis.png'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a51385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply Gaussian Blur -> reduces noise and smoothens the image to prevent false edges.\n",
    "img_gray_blur = cv2.GaussianBlur(src=img_gray, ksize=(5, 5), sigmaX=1.4)\n",
    "\n",
    "# Select the values of thesholds by trial & error.\n",
    "# detects edges by computing gradients and applying non-maximum suppression followed by hysteresis thresholding.\n",
    "edges = cv2.Canny(image=img_gray_blur, threshold1=100, threshold2=200)\n",
    "cv2.imshow('Canny Edge-Detection', edges)\n",
    "\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d83cc81",
   "metadata": {},
   "source": [
    "#### Morphological operations are image processing techniques that process images based on their shapes. These operations are typically applied to binary images but can also work on grayscale images. They are used to remove noise, separate objects, or enhance certain structures in an image.\n",
    "\n",
    "1. Erosion:\n",
    "   _Shrinks the white regions (foreground) in an image. Removes small noise by eroding boundaries of objects._\n",
    "\n",
    "2. Dilation:\n",
    "   _Expands the white regions (foreground) in an image. Useful for connecting broken parts of an object._\n",
    "\n",
    "3. Opening: Erosion then Dilation\n",
    "   _Removes small noise while preserving the shape of larger objects._\n",
    "\n",
    "4. Closing: Dilation then Erosion\n",
    "   _Fills small holes or gaps in the foreground._\n",
    "\n",
    "5. Morphological Gradient:\n",
    "   _The difference between dilation and erosion. Highlights the edges of objects._\n",
    "\n",
    "6. Top Hat:\n",
    "   _The difference between the input image and its opening. Useful for extracting small bright regions on a dark background._\n",
    "\n",
    "7. Black Hat:\n",
    "   _The difference between the closing of the image and the input image.Useful for extracting small dark regions on a bright background._\n",
    "\n",
    "#####\n",
    "\n",
    "==============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a02583d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('../view.jpg')\n",
    "cv2.imshow('BGR image', img)\n",
    "\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow('Gray-Scale image', img_gray)\n",
    "\n",
    "# Structuring Element = Kernel\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79e9c670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Erosion : cv2.erode(img,kernel,iterations = 1)\n",
    "\n",
    "cv2.imshow('Gray-Scale image', img_gray)\n",
    "\n",
    "erosion = cv2.erode(img_gray, kernel=kernel, iterations=1)\n",
    "cv2.imshow(\"Erosion\", erosion)\n",
    "\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c480a1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dilation : cv2.dilate(img,kernel,iterations = 1)\n",
    "\n",
    "cv2.imshow('Gray-Scale image', img_gray)\n",
    "\n",
    "dilation = cv2.dilate(img_gray, kernel=kernel, iterations=1)\n",
    "cv2.imshow(\"Dilation\", dilation)\n",
    "\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259b504c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Openning : cv2.morphologyEx(img, cv.MORPH_OPEN, kernel)\n",
    "# Opening (Erosion ➜ Dilation)\n",
    "\n",
    "cv2.imshow('Gray-Scale image', img_gray)\n",
    "\n",
    "open = cv2.morphologyEx(img_gray, cv2.MORPH_OPEN, kernel=kernel, iterations=1)\n",
    "cv2.imshow(\"Openning\", open)\n",
    "\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8bed3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Closing : cv2.morphologyEx(img, cv.MORPH_CLOSE, kernel)\n",
    "# Closing (Dilation ➜ Erosion)\n",
    "\n",
    "cv2.imshow('Gray-Scale image', img_gray)\n",
    "\n",
    "close = cv2.morphologyEx(img_gray, cv2.MORPH_CLOSE,\n",
    "                         kernel=kernel, iterations=1)\n",
    "cv2.imshow(\"Closing\", close)\n",
    "\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5defa935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Morphological Gradient : cv2.morphologyEx(img, cv.MORPH_GRADIENT, kernel)\n",
    "# Morphological Gradient (Dilation - Erosion)\n",
    "\n",
    "cv2.imshow('Gray-Scale image', img_gray)\n",
    "\n",
    "gradient = cv2.morphologyEx(\n",
    "    img_gray, cv2.MORPH_TOPHAT, kernel=kernel, iterations=1)\n",
    "cv2.imshow(\"Morphological Gradient\", gradient)\n",
    "\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3110b6e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top Hat : cv2.morphologyEx(img, cv.MORPH_TOPHAT, kernel)\n",
    "# Morphological Gradient (Dilation - Erosion)\n",
    "\n",
    "cv2.imshow('Gray-Scale image', img_gray)\n",
    "\n",
    "top_hat = cv2.morphologyEx(img_gray, cv2.MORPH_TOPHAT,\n",
    "                           kernel=kernel, iterations=1)\n",
    "cv2.imshow(\"Top Hat\", top_hat)\n",
    "\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb41297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Black Hat : cv2.morphologyEx(img, cv.MORPH_BLACKHAT, kernel)\n",
    "# Black Hat (Closing - Original)\n",
    "\n",
    "cv2.imshow('Gray-Scale image', img_gray)\n",
    "\n",
    "black_hat = cv2.morphologyEx(\n",
    "    img_gray, cv2.MORPH_BLACKHAT, kernel=kernel, iterations=1)\n",
    "cv2.imshow(\"Black Hat\", black_hat)\n",
    "\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4795b9",
   "metadata": {},
   "source": [
    "#### [Drawing](<(https://docs.opencv.org/4.x/dc/da5/tutorial_py_drawing_functions.html)>)\n",
    "\n",
    "_Learn to draw different geometric shapes with OpenCV_\n",
    "\n",
    "1. Line\n",
    "2. Rectangle\n",
    "3. Circle\n",
    "4. Text\n",
    "\n",
    "-\n",
    "\n",
    "1. Text data that you want to write\n",
    "2. Position coordinates of where you want put it (i.e. bottom-left corner where data starts).\n",
    "3. Font type (Check cv.putText() docs for supported fonts)\n",
    "4. Font Scale (specifies the size of font)\n",
    "5. regular things like color, thickness, lineType etc. For better look, lineType = cv.LINE_AA is recommended.\n",
    "\n",
    "- _Common arguments as given below:_\n",
    "\n",
    "-\n",
    "\n",
    "1. img : The image where you want to draw the shapes\n",
    "2. color : Color of the shape. for BGR, pass it as a tuple, eg: (255,0,0) for blue. For grayscale, just pass the scalar value.\n",
    "3. thickness : Thickness of the line or circle etc. If -1 is passed for closed figures like circles, it will fill the shape. default thickness = 1\n",
    "4. lineType : Type of line, whether 8-connected, anti-aliased line etc. By default, it is 8-connected. cv.LINE_AA gives anti-aliased line which looks great for curves.\n",
    "\n",
    "-\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0d1a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a black image\n",
    "img = np.zeros((512, 512, 3), np.uint8)\n",
    "cv2.imshow('Original', img)\n",
    "\n",
    "# Draw a diagonal blue line with thickness of 5 px\n",
    "line_img = cv2.line(img, (0, 0), (511, 511), (255, 0, 0), 5)\n",
    "cv2.imshow('Line', line_img)\n",
    "\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c354e5f0",
   "metadata": {},
   "source": [
    "_To draw the ellipse, we need to pass several arguments. One argument is_\n",
    "\n",
    "1. the center location (x,y).\n",
    "2. Next argument is axes lengths (major axis length, minor axis length).\n",
    "3. angle is the angle of rotation of ellipse in anti-clockwise direction.\n",
    "4. startAngle and endAngle denotes the starting and ending of ellipse arc measured in clockwise direction from major axis. i.e. giving values 0 and 360 gives the full ellipse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25be3016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ellipse = cv2.ellipse(img=img, center=(256, 256), axes=(\n",
    "    100, 50), angle=0, startAngle=0, endAngle=180, color=(0, 0, 255), thickness=-1)\n",
    "\n",
    "cv2.imshow('Ellipse', ellipse)\n",
    "\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0816c4",
   "metadata": {},
   "source": [
    "_To draw a polygon, first you need coordinates of vertices. Make those points into an array of shape ROWSx1x2 where ROWS are number of vertices and it should be of type int32. Here we draw a small polygon of with four vertices in yellow color._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa769380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pts = np.array([[10, 5], [20, 30]], np.int32)\n",
    "pts = pts.reshape((-1, 1, 2))\n",
    "\n",
    "poly = cv2.polylines(img=img, pts=[pts], isClosed=False, color=(0, 255, 0))\n",
    "\n",
    "cv2.imshow('Polygon', poly)\n",
    "\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7a6deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(198, 262, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board = cv2.imread('../board.png')\n",
    "cv2.imshow('Board', board)\n",
    "\n",
    "print(board.shape)\n",
    "\n",
    "# Line\n",
    "line_board = cv2.line(img=board, pt1=(50, 100), pt2=(\n",
    "    30, 150), color=(0, 255, 0), thickness=5)\n",
    "cv2.imshow('Line on Board', line_board)\n",
    "\n",
    "# Rectangle : you need top-left corner and bottom-right corner of rectangle\n",
    "rectangle_board = cv2.rectangle(img=board, pt1=(\n",
    "    50, 70), pt2=(100, 150), color=(255, 255, 0), thickness=2)\n",
    "cv2.imshow('Rectangle on Board', rectangle_board)\n",
    "\n",
    "# Circle : you need its center coordinates and radius\n",
    "circle_board = cv2.circle(img=board, center=(\n",
    "    50, 100), radius=20, color=(0, 0, 255), thickness=5)\n",
    "cv2.imshow('Circle on Board', circle_board)\n",
    "\n",
    "# Text\n",
    "text = \"I am Ahmed Akram\"\n",
    "text_board = cv2.putText(img=board, text=text, org=(30, 70), fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                         fontScale=0.5,\n",
    "                         color=(255, 255, 255),\n",
    "                         thickness=2)\n",
    "cv2.imshow('Text on Board', text_board)\n",
    "\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e13592e",
   "metadata": {},
   "source": [
    "#### [Contours](https://docs.opencv.org/3.4/d4/d73/tutorial_py_contours_begin.html)\n",
    "\n",
    "_Contours can be explained simply as a curve joining all the continuous points (along the boundary), having same color or intensity. The contours are a useful tool for shape analysis and object detection and recognition._\n",
    "\n",
    "_For better accuracy, use binary images. So before finding contours, apply threshold or canny edge detection.\n",
    "Since OpenCV 3.2, findContours() no longer modifies the source image but returns a modified image as the first of three return parameters.\n",
    "In OpenCV, finding contours is like finding white object from black background. So remember, object to be found should be white and background should be black._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d20bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "img = cv2.imread('../skybirds.png')\n",
    "cv2.imshow('Birds', img)\n",
    "\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow('Bird - Gray', img_gray)\n",
    "\n",
    "ret, thresh = cv2.threshold(img_gray, 90, 200, cv2.THRESH_BINARY)\n",
    "cv2.imshow('Threshold', thresh)\n",
    "\n",
    "\n",
    "contours, hierarchy = cv2.findContours(\n",
    "    image=thresh, mode=cv2.RETR_TREE, method=cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# contours is a Python list of all the contours in the image\n",
    "for cnt in contours:\n",
    "    if cv2.contourArea(cnt) > 150:\n",
    "        cv2.drawContours(thresh, cnt, -1, (0, 255, 0), 1)\n",
    "        x1, y1, w, h = cv2.boundingRect(cnt)\n",
    "        cv2.rectangle(img, (x1, y1), (x1 + w, y1 + h), (0, 255, 255), 2)\n",
    "\n",
    "cv2.imshow('Contours', img)\n",
    "\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7529d155",
   "metadata": {},
   "source": [
    "#### Human Vision\n",
    "\n",
    "##### Perception of Color\n",
    "\n",
    "_Human Eye has 3 Types of Photoreceptor cells for color -> Cones, Responsibility Spectra of human cone cells centered at blue, green and red & Trichromacy is a characterstic of humans and some animals_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7906f3c",
   "metadata": {},
   "source": [
    "#### Project 1 : Color Detector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c3ff358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd5f469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py\n",
    "\n",
    "# Helper Function\n",
    "def get_limits(color):\n",
    "    # Insert the BGR Values which you want to convert to HSV\n",
    "    c = np.uint8([[color]])\n",
    "    hsvC = cv2.cvtColor(c, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    lower_limit = hsvC[0][0][0] - 10, 100, 100\n",
    "    upper_limit = hsvC[0][0][0] + 10, 255, 255\n",
    "\n",
    "    lower_limit = np.array(lower_limit, dtype=np.uint8)\n",
    "    upper_limit = np.array(upper_limit, dtype=np.uint8)\n",
    "\n",
    "    return lower_limit, upper_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd854d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "yellow = [0, 255, 255]  # Yellow in BGR Colorspae\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    hsv_img = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    lower, upper = get_limits(color=yellow)\n",
    "    mask = cv2.inRange(hsv_img, lower, upper)\n",
    "\n",
    "    mask_ = Image.fromarray(mask)\n",
    "    bbox = mask_.getbbox()\n",
    "    # print(bbox)\n",
    "    if bbox is not None:\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 5)\n",
    "\n",
    "    cv2.imshow('Frame', frame)\n",
    "\n",
    "    if cv2.waitKey(40) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87311da7",
   "metadata": {},
   "source": [
    "#### Face Anonymizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621056d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
