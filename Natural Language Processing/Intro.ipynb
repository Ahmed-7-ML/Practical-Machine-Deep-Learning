{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5563b58c",
   "metadata": {},
   "source": [
    "### *NLP is a field of Computer Science and subfield of Artificial Intelligence, information engineering, and human-computer interaction. It focuses on how to process and analyze large amounts of natural language data efficiently. It is difficult to perform as the process of reading and understanding languages is far more complex than it seems at first glance.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7f9d57",
   "metadata": {},
   "source": [
    "*Free Available Pre-trained Models*:\n",
    "1. Fasttext\n",
    "2. Tensorflow Hub\n",
    "3. Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133c41d3",
   "metadata": {},
   "source": [
    "#### Information Extraction (IE)\n",
    "*Power of RegEx*\n",
    "- Sequence of characters that define a search pattern\n",
    "- Helpful Links : \n",
    "    - [Pythex]((https://pythex.org/))\n",
    "    - [regular expression 101](https://regex101.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8f5eb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e1cdd4",
   "metadata": {},
   "source": [
    "#### Tokenization :\n",
    "- Fundamental step in NLP Pipeline. \n",
    "- Divide a Textual (String) input into smaller units known as tokens.\n",
    "- Uses a tokenizer to segment unstructured data and natural language text into distinct chunks of information, treating them as different elements.\n",
    "- These tokens can be in the form of words, characters, sub-words, or sentences.\n",
    "- It helps in improving interpretability of text by different models.\n",
    "- Application: Multiple NLP tasks, text processing, language modelling, and machine translation.\n",
    "\n",
    "<img src='Tokenization-in-Natural-Language-Processing.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6167ed",
   "metadata": {},
   "source": [
    "*Types of Tokenization*:\n",
    "- Word Tokenization : text is divided into individual words. \n",
    "- Character Tokenization : the textual data is split and converted to a sequence of individual characters.\n",
    "- Sentence Tokenization : make a division of paragraphs or large set of sentences into separated sentences as tokens\n",
    "- Subword Tokenization : strikes a balance between word and character tokenization by breaking down text into units that are larger than a single character but smaller than a full word\n",
    "- N-gram Tokenization : splits words into fixed-sized chunks (size = n) of data\n",
    "\n",
    "*Popular Approaches of Tokenization*:\n",
    "- Whitespace Tokenization\n",
    "- Statistical Tokenization\n",
    "- Transformer-based Tokenization\n",
    "- Rule-based Tokenization\n",
    "- Byte-pair Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27401b6",
   "metadata": {},
   "source": [
    "*Sentence_Tokenizer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f2aac20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello everyone.',\n",
       " 'I am Ahmed Akram Amer.',\n",
       " 'I hope , i will be a successful.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence Tokenization using sent_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Hello everyone. I am Ahmed Akram Amer. I hope , i will be a successful.\"\n",
    "sent_tokenize(text=text, language='english')\n",
    "\n",
    "# It uses an instance of PunktSentenceTokenizer from the nltk.tokenize.punkt module,\n",
    "# which is already been trained and thus very well knows to mark the end and beginning of sentence at what characters and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello everyone.',\n",
       " 'I am Ahmed Akram Amer.',\n",
       " 'I hope , i will be a successful.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence Tokenization using PunktSentenceTokenize\n",
    "import nltk.data\n",
    "\n",
    "# Loading PunktSentenceTokenizer using English pickle file\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle')\n",
    "tokenizer.tokenize(text)\n",
    "\n",
    "# Sentences from different languages can also be tokenized using different pickle file other than English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00974a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hola amigo.', 'Estoy bien.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize a Spanish text into sentences using pre-trained Punkt tokenizer for Spanish.\n",
    "\n",
    "spanish_tokenizer = nltk.data.load('tokenizers/punkt/PY3/spanish.pickle')\n",
    "\n",
    "text = 'Hola amigo. Estoy bien.'\n",
    "spanish_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa409f19",
   "metadata": {},
   "source": [
    "*Word_Tokenizer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24db96d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', 'World', 'From', 'Egypt', '!', 'Hello', '!']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word Tokenization using work_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Hello, World From Egypt!\\nHello!\"\n",
    "word_tokenize(text=text, language='english', preserve_line=False)\n",
    "\n",
    "# it is a wrapper function that calls tokenize() on an instance of the TreebankWordTokenizer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38714e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H e l l o,   W o r l d   F r o m   E g y p t! \\n H e l l o!'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word Tokenization Using TreebankWordTokenizer \n",
    "from nltk.tokenize import TreebankWordDetokenizer\n",
    "\n",
    "tokenizer = TreebankWordDetokenizer()\n",
    "tokenizer.tokenize(text, convert_parentheses=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2bf86d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let', \"'\", 's', 'see', 'how', 'it', \"'\", 's', 'working', '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word Tokenization using WordPunctTokenizer\n",
    "# splits words based on punctuation boundaries.\n",
    "# Each punctuation mark is treated as a separate token.\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "tokenizer.tokenize(\"Let's see how it's working.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f08a28db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cristiano',\n",
       " 'Ronaldo',\n",
       " 'is',\n",
       " 'the',\n",
       " 'Best',\n",
       " 'Player',\n",
       " 'in',\n",
       " 'the',\n",
       " 'World',\n",
       " 'Hey',\n",
       " 'CR7']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word Tokenization using Regular Expression\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(pattern=r'\\w+')\n",
    "\n",
    "tokenizer.tokenize('Cristiano Ronaldo is the Best Player in the World, Hey CR7!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a29346",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
