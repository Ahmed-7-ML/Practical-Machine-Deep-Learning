{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcdd1354",
   "metadata": {},
   "source": [
    "[Topic Link](https://spacy.io/usage/linguistic-features#tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83e6003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d60ab8c",
   "metadata": {},
   "source": [
    "- *Important note: spaCy’s tokenization is `non-destructive`, which means that you’ll always be able to reconstruct the original input from the tokenized output. Whitespace information is preserved in the tokens and no information is added or removed during tokenization. This is kind of a core principle of spaCy’s `Doc object: doc.text == input_text` should always hold `true`.*\n",
    "\n",
    "- *During processing, spaCy first tokenizes the text, i.e. segments it into words, punctuation and so on. This is done by applying rules specific to each language. For example, punctuation at the end of a sentence should be split off – whereas “U.K.” should remain one token. Each Doc consists of individual tokens, and we can iterate over them:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1bee53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('San Francisco', 0, 13, 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"San Francisco considers banning sidewalk delivery robots\")\n",
    "\n",
    "# document level\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "print(ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3719cf45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['San', 'B', 'GPE']\n",
      "['Francisco', 'I', 'GPE']\n"
     ]
    }
   ],
   "source": [
    "# token level\n",
    "ent_san = [doc[0].text, doc[0].ent_iob_, doc[0].ent_type_]\n",
    "ent_francisco = [doc[1].text, doc[1].ent_iob_, doc[1].ent_type_]\n",
    "\n",
    "print(ent_san)\n",
    "print(ent_francisco)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa4abf0",
   "metadata": {},
   "source": [
    "*`Adding special case tokenization rules` : Most domains have at least some idiosyncrasies that require custom tokenization rules. This could be very certain expressions, or abbreviations only used in this specific field. Here’s how to add a special case rule to an existing Tokenizer instance*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f6209ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gimme', 'that']\n",
      "['gim', 'me', 'that']\n"
     ]
    }
   ],
   "source": [
    "from spacy.symbols import ORTH\n",
    "\n",
    "doc = nlp(\"gimme that\")  # phrase to tokenize\n",
    "print([w.text for w in doc])  # ['gimme', 'that']\n",
    "\n",
    "# Add special case rule\n",
    "special_case = [{ORTH: \"gim\"}, {ORTH: \"me\"}]\n",
    "nlp.tokenizer.add_special_case(\"gimme\", special_case)\n",
    "\n",
    "# Check new tokenization\n",
    "print([w.text for w in nlp(\"gimme that\")])  # ['gim', 'me', 'that']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b393bf",
   "metadata": {},
   "source": [
    "*The special case doesn’t have to match an entire whitespace-delimited substring. The tokenizer will incrementally split off punctuation, and keep looking up the remaining substring. The special case rules also have precedence over the punctuation splitting.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95473a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"gimme\" not in [w.text for w in nlp(\"gimme!\")]\n",
    "assert \"gimme\" not in [w.text for w in nlp('(\"...gimme...?\")')]\n",
    "\n",
    "nlp.tokenizer.add_special_case(\"...gimme...?\", [{\"ORTH\": \"...gimme...?\"}])\n",
    "assert len(nlp(\"...gimme...?\")) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abcbb69",
   "metadata": {},
   "source": [
    "*`Debugging the tokenizer' : A working implementation of the pseudo-code above is available for debugging as nlp.tokenizer.explain(text). It returns a list of tuples showing which tokenizer rule or pattern was matched for each token. The tokens produced are identical to nlp.tokenizer() except for whitespace tokens:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4361adbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" \\t PREFIX\n",
      "Let \\t SPECIAL-1\n",
      "'s \\t SPECIAL-2\n",
      "go \\t TOKEN\n",
      "! \\t SUFFIX\n",
      "\" \\t SUFFIX\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "text = '''\"Let's go!\"'''\n",
    "doc = nlp(text)\n",
    "tok_exp = nlp.tokenizer.explain(text)\n",
    "assert [t.text for t in doc if not t.is_space] == [t[1] for t in tok_exp]\n",
    "for t in tok_exp:\n",
    "    print(t[1], \"\\\\t\", t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb54a7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', '-', 'world.', ':)']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "special_cases = {\":)\": [{\"ORTH\": \":)\"}]}\n",
    "prefix_re = re.compile(r'''^[\\\\[\\\\(\"']''')\n",
    "# suffix_re = re.compile(r'''[\\\\]\\\\)\"']$''')\n",
    "infix_re = re.compile(r'''[-~]''')\n",
    "simple_url_re = re.compile(r'''^https?://''')\n",
    "\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    return Tokenizer(nlp.vocab, rules=special_cases,prefix_search=prefix_re.search,# suffix_search=suffix_re.search,\n",
    "                    infix_finditer=infix_re.finditer,url_match=simple_url_re.match)\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "doc = nlp(\"hello-world. :)\")\n",
    "print([t.text for t in doc])  # ['hello', '-', 'world.', ':)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8287a27b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
