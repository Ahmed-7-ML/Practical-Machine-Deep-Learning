{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM8kLxUEVc3Z"
      },
      "source": [
        "# Natural Language Processing Demystified | Preprocessing\n",
        "https://nlpdemystified.org<br>\n",
        "https://github.com/futuremojo/nlp-demystified"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btimL_w92Q3P"
      },
      "source": [
        "### spaCy upgrade and package installation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7Ll-fUK2VZs"
      },
      "source": [
        "At the time this notebook was created, spaCy had newer releases but Colab was still using version 2.x by default. So the first step is to upgrade spaCy.\n",
        "<br><br>\n",
        "**IMPORTANT**<br>\n",
        "If you're running this in the cloud rather than using a local Jupyter server on your machine, then the notebook will **timeout** after a period of inactivity. If that happens and you don't reconnect in time, you will need to upgrade spaCy again and reinstall the requisite statistical packages.\n",
        "<br><br>\n",
        "Refer to this link on how to run Colab notebooks locally on your machine to avoid this issue:<br>\n",
        "https://research.google.com/colaboratory/local-runtimes.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cve1-G7j2VTN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting spacy==3.*\n",
            "  Downloading spacy-3.8.7-cp313-cp313-win_amd64.whl.metadata (28 kB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy==3.*)\n",
            "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy==3.*)\n",
            "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy==3.*)\n",
            "  Downloading murmurhash-1.0.13-cp313-cp313-win_amd64.whl.metadata (2.2 kB)\n",
            "Collecting cymem<2.1.0,>=2.0.2 (from spacy==3.*)\n",
            "  Downloading cymem-2.0.11-cp313-cp313-win_amd64.whl.metadata (8.8 kB)\n",
            "Collecting preshed<3.1.0,>=3.0.2 (from spacy==3.*)\n",
            "  Downloading preshed-3.0.10-cp313-cp313-win_amd64.whl.metadata (2.5 kB)\n",
            "Collecting thinc<8.4.0,>=8.3.4 (from spacy==3.*)\n",
            "  Downloading thinc-8.3.6-cp313-cp313-win_amd64.whl.metadata (15 kB)\n",
            "Collecting wasabi<1.2.0,>=0.9.1 (from spacy==3.*)\n",
            "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.3 (from spacy==3.*)\n",
            "  Downloading srsly-2.5.1-cp313-cp313-win_amd64.whl.metadata (20 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.6 (from spacy==3.*)\n",
            "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting weasel<0.5.0,>=0.1.0 (from spacy==3.*)\n",
            "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\mr\\anaconda3\\lib\\site-packages (from spacy==3.*) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\mr\\anaconda3\\lib\\site-packages (from spacy==3.*) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\mr\\anaconda3\\lib\\site-packages (from spacy==3.*) (2.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\mr\\anaconda3\\lib\\site-packages (from spacy==3.*) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\mr\\anaconda3\\lib\\site-packages (from spacy==3.*) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\mr\\anaconda3\\lib\\site-packages (from spacy==3.*) (3.1.6)\n",
            "Requirement already satisfied: setuptools in c:\\users\\mr\\anaconda3\\lib\\site-packages (from spacy==3.*) (72.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\mr\\anaconda3\\lib\\site-packages (from spacy==3.*) (24.2)\n",
            "Collecting langcodes<4.0.0,>=3.2.0 (from spacy==3.*)\n",
            "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy==3.*)\n",
            "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\mr\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.*) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\mr\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.*) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\mr\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.*) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mr\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.*) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mr\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.*) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mr\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.*) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mr\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.*) (2025.4.26)\n",
            "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy==3.*)\n",
            "  Downloading blis-1.3.0-cp313-cp313-win_amd64.whl.metadata (7.6 kB)\n",
            "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy==3.*)\n",
            "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\mr\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy==3.*) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\mr\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy==3.*) (8.1.8)\n",
            "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy==3.*)\n",
            "  Downloading cloudpathlib-0.22.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy==3.*)\n",
            "  Downloading smart_open-7.3.0.post1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: wrapt in c:\\users\\mr\\anaconda3\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy==3.*) (1.17.0)\n",
            "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.*)\n",
            "  Downloading marisa_trie-1.3.1-cp313-cp313-win_amd64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mr\\anaconda3\\lib\\site-packages (from jinja2->spacy==3.*) (3.0.2)\n",
            "Downloading spacy-3.8.7-cp313-cp313-win_amd64.whl (13.9 MB)\n",
            "   ---------------------------------------- 0.0/13.9 MB ? eta -:--:--\n",
            "    --------------------------------------- 0.3/13.9 MB ? eta -:--:--\n",
            "   -- ------------------------------------- 0.8/13.9 MB 2.1 MB/s eta 0:00:07\n",
            "   --- ------------------------------------ 1.0/13.9 MB 2.1 MB/s eta 0:00:07\n",
            "   ---- ----------------------------------- 1.6/13.9 MB 2.1 MB/s eta 0:00:07\n",
            "   ----- ---------------------------------- 1.8/13.9 MB 2.1 MB/s eta 0:00:06\n",
            "   ------ --------------------------------- 2.4/13.9 MB 2.1 MB/s eta 0:00:06\n",
            "   -------- ------------------------------- 2.9/13.9 MB 2.0 MB/s eta 0:00:06\n",
            "   --------- ------------------------------ 3.1/13.9 MB 2.0 MB/s eta 0:00:06\n",
            "   ---------- ----------------------------- 3.7/13.9 MB 2.0 MB/s eta 0:00:05\n",
            "   ------------ --------------------------- 4.2/13.9 MB 2.0 MB/s eta 0:00:05\n",
            "   ------------ --------------------------- 4.5/13.9 MB 2.0 MB/s eta 0:00:05\n",
            "   -------------- ------------------------- 5.0/13.9 MB 2.0 MB/s eta 0:00:05\n",
            "   --------------- ------------------------ 5.5/13.9 MB 2.0 MB/s eta 0:00:05\n",
            "   ---------------- ----------------------- 5.8/13.9 MB 2.0 MB/s eta 0:00:04\n",
            "   ----------------- ---------------------- 6.0/13.9 MB 2.0 MB/s eta 0:00:04\n",
            "   ----------------- ---------------------- 6.0/13.9 MB 2.0 MB/s eta 0:00:04\n",
            "   -------------------- ------------------- 7.1/13.9 MB 2.0 MB/s eta 0:00:04\n",
            "   --------------------- ------------------ 7.6/13.9 MB 2.0 MB/s eta 0:00:04\n",
            "   ---------------------- ----------------- 7.9/13.9 MB 2.0 MB/s eta 0:00:03\n",
            "   ------------------------ --------------- 8.4/13.9 MB 2.0 MB/s eta 0:00:03\n",
            "   ------------------------- -------------- 8.9/13.9 MB 2.0 MB/s eta 0:00:03\n",
            "   -------------------------- ------------- 9.2/13.9 MB 2.0 MB/s eta 0:00:03\n",
            "   --------------------------- ------------ 9.7/13.9 MB 2.0 MB/s eta 0:00:03\n",
            "   ----------------------------- ---------- 10.2/13.9 MB 2.0 MB/s eta 0:00:02\n",
            "   ------------------------------ --------- 10.5/13.9 MB 2.0 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 11.0/13.9 MB 2.0 MB/s eta 0:00:02\n",
            "   --------------------------------- ------ 11.5/13.9 MB 2.0 MB/s eta 0:00:02\n",
            "   --------------------------------- ------ 11.8/13.9 MB 2.0 MB/s eta 0:00:02\n",
            "   ----------------------------------- ---- 12.3/13.9 MB 2.0 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 12.8/13.9 MB 2.0 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 13.1/13.9 MB 2.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------  13.6/13.9 MB 2.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 13.9/13.9 MB 2.0 MB/s eta 0:00:00\n",
            "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
            "Downloading cymem-2.0.11-cp313-cp313-win_amd64.whl (39 kB)\n",
            "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
            "Downloading murmurhash-1.0.13-cp313-cp313-win_amd64.whl (24 kB)\n",
            "Downloading preshed-3.0.10-cp313-cp313-win_amd64.whl (115 kB)\n",
            "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
            "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
            "Downloading srsly-2.5.1-cp313-cp313-win_amd64.whl (630 kB)\n",
            "   ---------------------------------------- 0.0/630.6 kB ? eta -:--:--\n",
            "   ---------------- ----------------------- 262.1/630.6 kB ? eta -:--:--\n",
            "   ---------------------------------------- 630.6/630.6 kB 1.8 MB/s eta 0:00:00\n",
            "Downloading thinc-8.3.6-cp313-cp313-win_amd64.whl (1.7 MB)\n",
            "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
            "   ------ --------------------------------- 0.3/1.7 MB ? eta -:--:--\n",
            "   ------------------ --------------------- 0.8/1.7 MB 2.1 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 1.0/1.7 MB 2.1 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 1.0/1.7 MB 2.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.7/1.7 MB 1.9 MB/s eta 0:00:00\n",
            "Downloading blis-1.3.0-cp313-cp313-win_amd64.whl (6.3 MB)\n",
            "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
            "   - -------------------------------------- 0.3/6.3 MB ? eta -:--:--\n",
            "   ----- ---------------------------------- 0.8/6.3 MB 2.1 MB/s eta 0:00:03\n",
            "   ------ --------------------------------- 1.0/6.3 MB 2.1 MB/s eta 0:00:03\n",
            "   ---------- ----------------------------- 1.6/6.3 MB 2.1 MB/s eta 0:00:03\n",
            "   ------------- -------------------------- 2.1/6.3 MB 2.1 MB/s eta 0:00:03\n",
            "   --------------- ------------------------ 2.4/6.3 MB 2.0 MB/s eta 0:00:02\n",
            "   ------------------ --------------------- 2.9/6.3 MB 2.0 MB/s eta 0:00:02\n",
            "   -------------------- ------------------- 3.1/6.3 MB 2.0 MB/s eta 0:00:02\n",
            "   ----------------------- ---------------- 3.7/6.3 MB 2.0 MB/s eta 0:00:02\n",
            "   -------------------------- ------------- 4.2/6.3 MB 2.0 MB/s eta 0:00:02\n",
            "   ---------------------------- ----------- 4.5/6.3 MB 2.0 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 5.0/6.3 MB 2.0 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 5.5/6.3 MB 2.0 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 5.8/6.3 MB 2.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 6.3/6.3 MB 2.0 MB/s eta 0:00:00\n",
            "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
            "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
            "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
            "Downloading cloudpathlib-0.22.0-py3-none-any.whl (61 kB)\n",
            "Downloading smart_open-7.3.0.post1-py3-none-any.whl (61 kB)\n",
            "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
            "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
            "   - -------------------------------------- 0.3/5.4 MB ? eta -:--:--\n",
            "   ----- ---------------------------------- 0.8/5.4 MB 2.0 MB/s eta 0:00:03\n",
            "   ------- -------------------------------- 1.0/5.4 MB 2.1 MB/s eta 0:00:03\n",
            "   ----------- ---------------------------- 1.6/5.4 MB 2.0 MB/s eta 0:00:02\n",
            "   --------------- ------------------------ 2.1/5.4 MB 2.0 MB/s eta 0:00:02\n",
            "   ----------------- ---------------------- 2.4/5.4 MB 2.0 MB/s eta 0:00:02\n",
            "   --------------------- ------------------ 2.9/5.4 MB 2.0 MB/s eta 0:00:02\n",
            "   ------------------------- -------------- 3.4/5.4 MB 2.0 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 3.7/5.4 MB 2.0 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 4.2/5.4 MB 2.0 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 4.7/5.4 MB 2.0 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 5.0/5.4 MB 2.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 5.4/5.4 MB 2.0 MB/s eta 0:00:00\n",
            "Downloading marisa_trie-1.3.1-cp313-cp313-win_amd64.whl (139 kB)\n",
            "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, smart-open, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, srsly, preshed, language-data, langcodes, confection, weasel, thinc, spacy\n",
            "\n",
            "   -- -------------------------------------  1/18 [wasabi]\n",
            "   ------ ---------------------------------  3/18 [spacy-legacy]\n",
            "   -------- -------------------------------  4/18 [smart-open]\n",
            "   --------------- ------------------------  7/18 [cloudpathlib]\n",
            "   ----------------- ----------------------  8/18 [catalogue]\n",
            "   -------------------- -------------------  9/18 [blis]\n",
            "   ---------------------- ----------------- 10/18 [srsly]\n",
            "   ---------------------- ----------------- 10/18 [srsly]\n",
            "   ---------------------- ----------------- 10/18 [srsly]\n",
            "   ---------------------- ----------------- 10/18 [srsly]\n",
            "   ---------------------- ----------------- 10/18 [srsly]\n",
            "   ---------------------- ----------------- 10/18 [srsly]\n",
            "   ------------------------ --------------- 11/18 [preshed]\n",
            "   -------------------------- ------------- 12/18 [language-data]\n",
            "   -------------------------- ------------- 12/18 [language-data]\n",
            "   -------------------------- ------------- 12/18 [language-data]\n",
            "   -------------------------- ------------- 12/18 [language-data]\n",
            "   -------------------------- ------------- 12/18 [language-data]\n",
            "   -------------------------- ------------- 12/18 [language-data]\n",
            "   -------------------------- ------------- 12/18 [language-data]\n",
            "   -------------------------- ------------- 12/18 [language-data]\n",
            "   -------------------------- ------------- 12/18 [language-data]\n",
            "   -------------------------- ------------- 12/18 [language-data]\n",
            "   ---------------------------- ----------- 13/18 [langcodes]\n",
            "   ------------------------------- -------- 14/18 [confection]\n",
            "   ------------------------------- -------- 14/18 [confection]\n",
            "   --------------------------------- ------ 15/18 [weasel]\n",
            "   --------------------------------- ------ 15/18 [weasel]\n",
            "   ----------------------------------- ---- 16/18 [thinc]\n",
            "   ----------------------------------- ---- 16/18 [thinc]\n",
            "   ----------------------------------- ---- 16/18 [thinc]\n",
            "   ----------------------------------- ---- 16/18 [thinc]\n",
            "   ----------------------------------- ---- 16/18 [thinc]\n",
            "   ----------------------------------- ---- 16/18 [thinc]\n",
            "   ----------------------------------- ---- 16/18 [thinc]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ------------------------------------- -- 17/18 [spacy]\n",
            "   ---------------------------------------- 18/18 [spacy]\n",
            "\n",
            "Successfully installed blis-1.3.0 catalogue-2.0.10 cloudpathlib-0.22.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.3.1 murmurhash-1.0.13 preshed-3.0.10 smart-open-7.3.0.post1 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.6 wasabi-1.1.3 weasel-0.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U spacy==3.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "z-FDdbc62VHd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m\n",
            "============================== Info about spaCy ==============================\u001b[0m\n",
            "\n",
            "spaCy version    3.8.7                         \n",
            "Location         c:\\Users\\mr\\anaconda3\\Lib\\site-packages\\spacy\n",
            "Platform         Windows-10-10.0.19045-SP0     \n",
            "Python version   3.13.5                        \n",
            "Pipelines                                      \n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8vW9svTE289D"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfJKSJEU2U_s"
      },
      "source": [
        "After importing spaCy, the next thing we need to do is load a suitable statistical model for our project. spaCy offers a variety of models for different languages. These models help with tokenization, part-of-speech tagging, named entity recognition, and more.\n",
        "\n",
        "Here, we're loading the **en_core_web_sm** model which is the smallest English model spaCy offers and is a good starting point for NLP tasks.<br>\n",
        "https://spacy.io/models/en#en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v6TGQff2iu6"
      },
      "source": [
        "Since we upgraded spaCy, we'll need to download the statistical model as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4uOyHDNb2i5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "     - -------------------------------------- 0.5/12.8 MB 2.1 MB/s eta 0:00:06\n",
            "     --- ------------------------------------ 1.0/12.8 MB 2.0 MB/s eta 0:00:06\n",
            "     ---- ----------------------------------- 1.3/12.8 MB 2.0 MB/s eta 0:00:06\n",
            "     ---- ----------------------------------- 1.3/12.8 MB 2.0 MB/s eta 0:00:06\n",
            "     ------- -------------------------------- 2.4/12.8 MB 2.1 MB/s eta 0:00:06\n",
            "     -------- ------------------------------- 2.6/12.8 MB 2.0 MB/s eta 0:00:06\n",
            "     --------- ------------------------------ 3.1/12.8 MB 2.0 MB/s eta 0:00:05\n",
            "     ---------- ----------------------------- 3.4/12.8 MB 2.0 MB/s eta 0:00:05\n",
            "     ------------ --------------------------- 3.9/12.8 MB 2.0 MB/s eta 0:00:05\n",
            "     ------------- -------------------------- 4.5/12.8 MB 2.0 MB/s eta 0:00:05\n",
            "     -------------- ------------------------- 4.7/12.8 MB 2.0 MB/s eta 0:00:04\n",
            "     --------------- ------------------------ 5.0/12.8 MB 2.0 MB/s eta 0:00:04\n",
            "     ----------------- ---------------------- 5.5/12.8 MB 2.0 MB/s eta 0:00:04\n",
            "     ------------------- -------------------- 6.3/12.8 MB 2.0 MB/s eta 0:00:04\n",
            "     -------------------- ------------------- 6.6/12.8 MB 2.0 MB/s eta 0:00:04\n",
            "     --------------------- ------------------ 6.8/12.8 MB 2.0 MB/s eta 0:00:03\n",
            "     ---------------------- ----------------- 7.3/12.8 MB 2.0 MB/s eta 0:00:03\n",
            "     ------------------------ --------------- 7.9/12.8 MB 2.0 MB/s eta 0:00:03\n",
            "     -------------------------- ------------- 8.4/12.8 MB 2.0 MB/s eta 0:00:03\n",
            "     --------------------------- ------------ 8.7/12.8 MB 2.0 MB/s eta 0:00:03\n",
            "     ---------------------------- ----------- 9.2/12.8 MB 2.0 MB/s eta 0:00:02\n",
            "     ----------------------------- ---------- 9.4/12.8 MB 2.0 MB/s eta 0:00:02\n",
            "     ------------------------------ --------- 9.7/12.8 MB 2.0 MB/s eta 0:00:02\n",
            "     ------------------------------- -------- 10.2/12.8 MB 2.0 MB/s eta 0:00:02\n",
            "     ------------------------------- -------- 10.2/12.8 MB 2.0 MB/s eta 0:00:02\n",
            "     --------------------------------- ------ 10.7/12.8 MB 1.9 MB/s eta 0:00:02\n",
            "     ----------------------------------- ---- 11.3/12.8 MB 1.9 MB/s eta 0:00:01\n",
            "     ------------------------------------ --- 11.8/12.8 MB 1.9 MB/s eta 0:00:01\n",
            "     ------------------------------------- -- 12.1/12.8 MB 1.9 MB/s eta 0:00:01\n",
            "     ---------------------------------------  12.6/12.8 MB 2.0 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 12.8/12.8 MB 1.9 MB/s eta 0:00:00\n",
            "Installing collected packages: en-core-web-sm\n",
            "Successfully installed en-core-web-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mWDrpxDk2_r2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'spacy.lang.en.English'>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<spacy.lang.en.English at 0x18d49d70d70>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "print(type(nlp))\n",
        "nlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7YCbWtG3LJO"
      },
      "source": [
        "**en_core_web_sm** is trained on OntoNotes 5 which is an annotated corpus comprising news, blogs, transcripts, etc. Put simply, this means a bunch of documents were labelled with information such as how each sentence should be parsed, whether a particular word is a noun or adjective or other part-of-speech, whether a word is a special entity like a person or a real-world organization, and other language-related labels. A statistical model was then generated from these labelled documents.<br>\n",
        "https://catalog.ldc.upenn.edu/LDC2013T19\n",
        "<br><br>\n",
        "You can learn more about the available spaCy models at these links:<br>\n",
        "https://spacy.io/models<br>\n",
        "https://spacy.io/usage/models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvF_udvi3OTO"
      },
      "source": [
        "After loading the model, the _nlp_ variable now references a **Language** class instance which contains language-specific rules for various tasks (e.g. tokenization) and a processing pipeline.<br>\n",
        "https://spacy.io/api/language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DAYGtQpT3UNN"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "spacy.lang.en.English"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(nlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unmnGRu8D-wa"
      },
      "source": [
        "# Tokenization\n",
        "\n",
        "Course module for this demo:\n",
        "https://www.nlpdemystified.org/course/tokenization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLUcGm3IbQki"
      },
      "source": [
        "### Tokenization with spaCy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13twUCp2i_p8"
      },
      "source": [
        "We pass whatever text we want to process to _nlp_, which returns a **Doc** container object containing the tokenized text and a number of annotations for each token. These annotations are discussed in follow-up videos. You can learn more about the **Doc** object here:<br>\n",
        "https://spacy.io/api/doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BIoEJZ-IkHQ4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'spacy.tokens.doc.Doc'>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "He didn't want to pay $20 for this book."
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Sample sentence.\n",
        "s = \"He didn't want to pay $20 for this book.\"\n",
        "doc = nlp(s)\n",
        "print(type(doc))\n",
        "doc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMWZK3ZSk9-f"
      },
      "source": [
        "We can iterate over this **Doc** object and view the tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8SzqhZuulAe1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['He', 'did', \"n't\", 'want', 'to', 'pay', '$', '20', 'for', 'this', 'book', '.']\n"
          ]
        }
      ],
      "source": [
        "print([t.text for t in doc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ai1obkB93GdD"
      },
      "source": [
        "Note how\n",
        "- \"didn't\" is separated into \"did\"  and \"n't\".\n",
        "- the currency symbol and amount are separated.\n",
        "- the period at the end of the sentence is its own token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWH49gIh3hqN"
      },
      "source": [
        "The **Doc** object can be indexed and sliced like a regular list. The **Doc** object contains **Token** and **Span** objects, which offer different views into the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MwLrxRsE3oKI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "He\n"
          ]
        }
      ],
      "source": [
        "# We can view an individual token by indexing into the Doc object.\n",
        "print(doc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bGapNHYQFYVa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'spacy.tokens.token.Token'>\n"
          ]
        }
      ],
      "source": [
        "# A Doc object is a container of other objects, namely Token and Span objects.\n",
        "print(type(doc[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EtL2IgIAGOd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "He didn't\n",
            "<class 'spacy.tokens.span.Span'>\n"
          ]
        }
      ],
      "source": [
        "# Slicing a Doc object returns a Span object.\n",
        "print(doc[0:3])\n",
        "print(type(doc[0:3]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "xybH4jjYGo73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('He', 0), ('did', 1), (\"n't\", 2), ('want', 3), ('to', 4), ('pay', 5), ('$', 6), ('20', 7), ('for', 8), ('this', 9), ('book', 10), ('.', 11)]\n"
          ]
        }
      ],
      "source": [
        "# Access a token's index in a sentence.\n",
        "print([(t.text, t.i) for t in doc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TqE980F4Vrt"
      },
      "source": [
        "Spacy's tokenization is _non-destructive_, which means the original input can be reconstructed from the tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "OjXb8mR_DK-1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "He didn't want to pay $20 for this book.\n"
          ]
        }
      ],
      "source": [
        "# You can view the original input like so:\n",
        "print(doc.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73vuSX7MDK79"
      },
      "source": [
        "You can learn more about the **Token** and **Span** objects here:<br>\n",
        "https://spacy.io/api/token<br>\n",
        "https://spacy.io/api/span\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lume_1UP6ySQ"
      },
      "source": [
        "We can also tokenize multiple sentences and access each sentence individually using the **Doc** object's _sents_ property."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mPZ86x0hDK4m"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Either the well was very deep, or she fell very slowly, for she\n",
            "had plenty of time as she went down to look about her and to wonder what\n",
            "was going to happen next., First, she tried to look down and make out what\n",
            "she was coming to, but it was too dark to see anything; then she looked at\n",
            "the sides of the well, and noticed that they were filled with cupboards and\n",
            "book-shelves; here and there she saw maps and pictures hung upon pegs.]\n"
          ]
        }
      ],
      "source": [
        "s = \"\"\"Either the well was very deep, or she fell very slowly, for she\n",
        "had plenty of time as she went down to look about her and to wonder what\n",
        "was going to happen next. First, she tried to look down and make out what\n",
        "she was coming to, but it was too dark to see anything; then she looked at\n",
        "the sides of the well, and noticed that they were filled with cupboards and\n",
        "book-shelves; here and there she saw maps and pictures hung upon pegs.\"\"\"\n",
        "\n",
        "doc = nlp(s)\n",
        "\n",
        "# Look at individual sentences (there should be two 'Span' objects).\n",
        "print([sent for sent in doc.sents])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n"
          ]
        }
      ],
      "source": [
        "print(len([sent for sent in doc.sents]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvSfDUyK06Qg"
      },
      "source": [
        "### Tokenization Exercises"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fyywcBrCHzSk"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "He didn't want to pay $20 for this book."
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# EXERCISE:\n",
        "# 1) Tokenize the following text\n",
        "# 2) Iterate through the tokens to check whether there's a currency symbol.\n",
        "# 3) If there is, and the currency label is followed by a number, print\n",
        "#    both the symbol and the number.\n",
        "#\n",
        "# Look through https://spacy.io/api/token#attributes on how to check whether\n",
        "# a token is a currency symbol or a number.\n",
        "#\n",
        "# Expected output: \"$20\".\n",
        "s = \"He didn't want to pay $20 for this book.\"\n",
        "doc = nlp(s)\n",
        "doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "$20\n"
          ]
        }
      ],
      "source": [
        "for i, token in enumerate(doc):\n",
        "    # Check if token is a currency symbol\n",
        "    if token.is_currency:\n",
        "        # Check if next token exists and is a digit/number\n",
        "        if i+1 < len(doc) and doc[i+1].like_num:\n",
        "            print(token.text+doc[i+1].text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "skajI-OZDK0t"
      },
      "outputs": [],
      "source": [
        "# EXERCISE: Learn how the spaCy tokenizer works and how to customize it:\n",
        "# https://spacy.io/usage/linguistic-features#tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ikbnyb8rDKv9"
      },
      "outputs": [],
      "source": [
        "# EXERCISE: Read through spaCy-101 and if you're interested, check out their course\n",
        "# on spaCy itself (link on the page).\n",
        "# https://spacy.io/usage/spacy-101"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMArLP91DKUW"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# EXERCISE: Look up how to tokenize the sentence below using NLTK. The imports\n",
        "# are done for you. Does the NLTK tokenizer handle \"N.Y.C.\" correctly?\n",
        "#\n",
        "import nltk\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "s = \"Let's go to N.Y.C. for the weekend.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMbm9tTakDdy"
      },
      "source": [
        "**NOTE**: Different tokenizers will give subtly different results based on the rules they use. Experiment with different tokenizers and use the one best suited for your project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUsfYCpVT4nI"
      },
      "source": [
        "# Basic Preprocessing\n",
        "## Case-Folding, Stop Word Removal, Stemming, and Lemmatization.\n",
        "\n",
        "Course module for this demo:\n",
        "https://www.nlpdemystified.org/course/basic-preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gaj23tgd7Su"
      },
      "source": [
        "**NOTE: If the notebook timed out, you may need to re-upgrade spaCy and re-install the language model as follows:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mg6dga4JePf2"
      },
      "outputs": [],
      "source": [
        "!pip install -U spacy==3.*\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgDDrCeI8f-4"
      },
      "source": [
        "spaCy performs all these preprocessing steps (except stemming) behind the scenes for you. Inline with its non-destructive policy, the tokens aren't modified directly. Rather, each **Token** object has a number of attributes which can help you get views of your document with these pre-processing steps applied. The attributes a **Token** has can be found here:<br>\n",
        "https://spacy.io/api/token#attributes\n",
        "<br><br>\n",
        "More information about spaCy's processing pipeline:<br>\n",
        "https://spacy.io/usage/processing-pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDEMR6En1j3H"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "s = \"He told Dr. Lovato that he was done with the tests and would post the results shortly.\"\n",
        "doc = nlp(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwA1ct0obYlR"
      },
      "source": [
        "### Case-Folding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biBPWrVd9BrK"
      },
      "source": [
        "View your document with case-folding using the *lower_* attribute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nt4RpzdgQQL"
      },
      "outputs": [],
      "source": [
        "print([t.lower_ for t in doc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HL46I4sH9OMq"
      },
      "source": [
        "You can also apply conditions when generating these views. For example, we can skip case-folding if a token is the start of a sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IO0PQ8IFhOlZ"
      },
      "outputs": [],
      "source": [
        "print([t.lower_ if not t.is_sent_start else t for t in doc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7pTz8XJbmaT"
      },
      "source": [
        "### Stop Word Removal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZLqqmHa9cRx"
      },
      "source": [
        "spaCy comes with a default stop word list. To view your document with stop words removed, you can use the *is_stop* attribute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kvXbuDEhOxu"
      },
      "outputs": [],
      "source": [
        "# spaCy's default stop word list.\n",
        "print(nlp.Defaults.stop_words)\n",
        "print(len(nlp.Defaults.stop_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAS1xmgOhO5y"
      },
      "outputs": [],
      "source": [
        "print([t for t in doc if not t.is_stop])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPd1aiLrbqcK"
      },
      "source": [
        "### Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKidP32Y_qcE"
      },
      "source": [
        "It's similar with lemmatization. You can view your document with lemmatization applied through the *lemma_* attribute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhdRleESkzTu"
      },
      "outputs": [],
      "source": [
        "[(t.text, t.lemma_) for t in doc]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuaQJPjEjADE"
      },
      "source": [
        "### Basic Preprocessing Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uNdkuJqCA2A"
      },
      "source": [
        "spaCy doesn't support stemming natively. But for completeness, we can stem using **NLTK**. Specifically, we can use the *Snowball stemmer* which is an improved version of the *Porter stemmer*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HQzMurVB13l"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# EXERCISE: Find out how to intialize the SnowballStemmer, then tokenize\n",
        "# and stem the sentence below.\n",
        "#\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "s = 'He told Dr. Lovato that he was done with the tests and would post the results shortly.'\n",
        "\n",
        "# Initialize the stemmer here.\n",
        "\n",
        "\n",
        "# Tokenize, stem, and print the tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOXJI061npqN"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# EXERCISE: Find out how to add and remove your own stop words in spaCy. Add the\n",
        "# word 'told' as a stop word, test that it works, then remove it from\n",
        "# the stop word list.\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLcCYIy-lP1u"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# EXERCISE: Read up on how to add your own custom attributes to Token objects\n",
        "# and try adding one of your own.\n",
        "# https://spacy.io/usage/processing-pipelines#custom-components-attributes\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9HLYYUt1kOP"
      },
      "source": [
        "#Advanced Preprocessing\n",
        "\n",
        "## Part-of-Speech Tagging, Named Entity Recognition, and Parsing.\n",
        "\n",
        "Course module for this demo:\n",
        "https://www.nlpdemystified.org/course/advanced-preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfBqaH9feymn"
      },
      "source": [
        "**NOTE: If the notebook timed out, you may need to re-upgrade spaCy and re-install the language model as follows:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xOdySsre_sP"
      },
      "outputs": [],
      "source": [
        "!pip install -U spacy==3.*\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tr5SqjHwSWpI"
      },
      "source": [
        "spaCy performs Part-of-Speech (POS) tagging, Named Entity Recognition (NER), and parsing as part of its default pipeline in the *nlp* object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shgWRMCq1kmy"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "s = \"John watched an old movie at the cinema.\"\n",
        "doc = nlp(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwMQgciGb3or"
      },
      "source": [
        "### Part-of-Speech Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA9LDzULTW1_"
      },
      "source": [
        "POS tags can be accessed through the *pos_* attribute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-9YRcSZ1kqq"
      },
      "outputs": [],
      "source": [
        "[(t.text, t.pos_) for t in doc]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UZcgwejnYm8"
      },
      "source": [
        "To get a description for a POS tag, we can use _spacy.explain_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9SXNvnmnW5e"
      },
      "outputs": [],
      "source": [
        "spacy.explain('PROPN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_WbFDZ-Tqu9"
      },
      "source": [
        "The POS tags above are called *course-grained* tags. You can also access *fine-grained* tags through the *tag_* attribute. Fine-grained tags provide more detailed information about a token such as its tense and, if a word is a pronoun, what specific type of pronoun it is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Z5oDzNr1kt2"
      },
      "outputs": [],
      "source": [
        "[(t.text, t.tag_) for t in doc]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPOaN9yOUN-I"
      },
      "source": [
        "So **NNP** refers specifically to a _singular pronoun_, and **VBD** is a verb in *past tense*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnfLDxoG1kxf"
      },
      "outputs": [],
      "source": [
        "print(spacy.explain('NNP'))\n",
        "print(spacy.explain('VBD'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jte6K6HJb750"
      },
      "source": [
        "### Named Entity Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J2BjPyqWFEf"
      },
      "source": [
        "There are multiple ways to access named entities. One way is through the *ent_type_* attribute.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWjNrX6koNVj"
      },
      "outputs": [],
      "source": [
        "s = \"Volkswagen is developing an electric sedan which could potentially come to America next fall.\"\n",
        "doc = nlp(s)\n",
        "\n",
        "[(t.text, t.ent_type_) for t in doc]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJL4wfS6Wp9N"
      },
      "source": [
        "You can view spaCy's named entities annotations here:<br>\n",
        "https://spacy.io/api/annotation#named-entities\n",
        "\n",
        "or use _spacy.explain_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iu4OiPwDo9So"
      },
      "outputs": [],
      "source": [
        "spacy.explain('GPE')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7p8IcNGpBTP"
      },
      "source": [
        "You can also check if a token is an entity before printing it by checking whether the _ent_type_ (note the lack of trailing underscore) attribute is non-zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aBng8zdvjly"
      },
      "outputs": [],
      "source": [
        "print([(t.text, t.ent_type_) for t in doc if t.ent_type != 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lNS65_2XJIY"
      },
      "source": [
        "Another way is through the _ents_ property of the **Doc** object. Here, we iterate through _ents_ and print the entity itself and its label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSCzxs02vjdL"
      },
      "outputs": [],
      "source": [
        "print([(ent.text, ent.label_) for ent in doc.ents])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHfZmta8XX9Y"
      },
      "source": [
        "Note how \"next fall\" is outputted above as a single span when you use _ents_.\n",
        "<br><br>\n",
        "You can also access the positions of entities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSzwRD0MvjTN"
      },
      "outputs": [],
      "source": [
        "print([(ent.text, ent.label_, ent.start_char, ent.end_char) for ent in doc.ents])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvvQ9_7FdEHT"
      },
      "source": [
        "spaCy is bundled with visualizers for both parsing and named entities.<br>\n",
        "https://spacy.io/usage/visualizers\n",
        "<br><br>\n",
        "Here, we visualize the entities in our sample sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87eLywmVZCdw"
      },
      "outputs": [],
      "source": [
        "from spacy import displacy\n",
        "\n",
        "# We need to set the 'jupyter' variable to True in order to output\n",
        "# the visualization directly. Otherwise, you'll get raw HTML.\n",
        "displacy.render(doc, style='ent', jupyter=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkNmqelTwTLZ"
      },
      "source": [
        "For domain-specific corpora, an NER tagger may need to be further fine-tuned. Here, we may want _The Martian_ tagged as a \"FILM\" (assuming that's our goal)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bcIaah29MME"
      },
      "outputs": [],
      "source": [
        "s = \"Ridley Scott directed The Martian.\"\n",
        "doc = nlp(s)\n",
        "displacy.render(doc, style='ent', jupyter=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noGuG3JvcEfs"
      },
      "source": [
        "### Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppWrztdJeO3J"
      },
      "source": [
        "Let's first visualize a parse to make it easier to follow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrvfA1TEvjJT"
      },
      "outputs": [],
      "source": [
        "s = \"She enrolled in the course at the university.\"\n",
        "doc = nlp(s)\n",
        "\n",
        "# Note the 'style' argument is assigned a 'dep' flag this time around.\n",
        "displacy.render(doc, style='dep', jupyter=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRN7_SQ-fO5H"
      },
      "source": [
        "The visualization above is for a dependency parse (spaCy doesn't come with a constituency parser). For each pair of depencencies, spaCy visualizes the child (pointed to), the head (pointed from), and their relationship (the label arc). You can view the dependency annotations here:<br>\n",
        "https://spacy.io/api/annotation#dependency-parsing\n",
        "\n",
        "You can also use *spacy.explain* to get information on a particular annotation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvz1bLTZfqmv"
      },
      "outputs": [],
      "source": [
        "spacy.explain('nsubj')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCvHyqHggIpd"
      },
      "source": [
        "The dependency labels themselves can be accessed through the *dep_* attribute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iX_BgpMVoNaj"
      },
      "outputs": [],
      "source": [
        "[(t.text, t.dep_) for t in doc]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tt7zLq0ugR7O"
      },
      "source": [
        "Note how the word 'enrolled' is the _ROOT_.\n",
        "<br><br>\n",
        "But the labels above don't show how the words are related to each other (the arcs). To get a better idea, you can print the head of each dependency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X15EOIq0oNfF"
      },
      "outputs": [],
      "source": [
        "[(t.text, t.dep_, t.head.text) for t in doc]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFXPL37Rg2xm"
      },
      "source": [
        "### Using spaCy's Matcher to find patterns\n",
        "spaCy comes with a host of pattern-matching functionality. Beyond regex, spaCy can match on a variety of attributes such as POS tags, entity labels, lemmas, dependencies, entire phrases, and a lot more. You can learn more here:<br>\n",
        "https://spacy.io/usage/rule-based-matching<br>\n",
        "https://explosion.ai/demos/matcher\n",
        "<br><br>\n",
        "Here, we try to search for patterns that may be useful for a hospitality bot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6v4hVnYmJuaK"
      },
      "outputs": [],
      "source": [
        "# The general Matcher is one of multiple matcher objects\n",
        "# included with spaCy.\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "# We initialize the Matcher with the spaCy vocab object, which contains\n",
        "# words along with their labels and entities.\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "s = \"I want to book a hotel room.\"\n",
        "doc = nlp(s)\n",
        "\n",
        "# Patterns are expressed as an ordered sequence. Here, we're looking\n",
        "# to match occurrences starting with a 'book' string followed by\n",
        "# a determiner (DET) POS tag, then a noun POS tag.\n",
        "# The OP key marks the match as optional in some way.\n",
        "\n",
        "# Here, the DET POS (marked with '?') will match 0 or 1 times, and\n",
        "# the NOUN POS (marked with '+') will match 1 or more times.\n",
        "# See this link for more information:\n",
        "# https://spacy.io/usage/rule-based-matching#quantifiers\n",
        "pattern = [\n",
        "  {'TEXT': 'book'},\n",
        "  {'POS': 'DET', 'OP': '?'},\n",
        "  {'POS': 'NOUN', 'OP': '+'},\n",
        "]\n",
        "\n",
        "# We give our pattern a label and pass it to the matcher.\n",
        "matcher.add('USER_INTENT', [pattern])\n",
        "\n",
        "# Run the matcher over the doc.\n",
        "matches = matcher(doc)\n",
        "\n",
        "# For each match, the matcher returns a tuple specifying a match id, start,\n",
        "# and end of the match.\n",
        "print(\"Matches:\", [doc[start:end].text for match_id, start, end in matches])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dygcIKF9plib"
      },
      "source": [
        "The code above demonstrates the Matcher but is brittle.\n",
        "- What if \"book\" is capitalized?\n",
        "- What if a user types \"reserve\" instead of \"book\"?\n",
        "- How can we match on \"hotel room\" as a compound noun?\n",
        "- What if a user types \"book a flight and hotel room\"?\n",
        "\n",
        "Can you think of how you would handle these cases?\n",
        "<br><br>\n",
        "We could come up more rules to match different patterns, or perhaps just search for keywords based on POS and entities (e.g. a country) and present the user with a bunch of possible intentions and let them choose one, or have a bunch of different interpretation functions submit answers and select the most likely one based on what was historically accepted most often. We can also ask clarifying questions to narrow things down.\n",
        "<br><br>\n",
        "For example, for the last sentence, you could have a function scan through the **Doc** object's *noun_chunks* (phrases that have a noun as their head) and isolate keywords there along with potential conjunctions (e.g. \"and\").<br>\n",
        "https://spacy.io/usage/linguistic-features#noun-chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xctXGD5K5Gvr"
      },
      "outputs": [],
      "source": [
        "doc = nlp(\"I want to book a flight and hotel room in Berlin.\")\n",
        "for noun_phrase in doc.noun_chunks:\n",
        "  print(\"phrase: {}, root head: {}\".format(noun_phrase, noun_phrase.root.head))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMsHWX-9EvXU"
      },
      "source": [
        "Using pure rules is a good place to start or prototype (especially if the domain is narrow with a tight set of use cases) but as our requirements get more sophisticated, we'll need to blend in other approaches such as classical models or perhaps deep learning (at the very least, maybe tune existing neural networks). spaCy's models can be updated with more examples to fine-tune predictions.<br>\n",
        "https://spacy.io/usage/training<br>\n",
        "<br>\n",
        "We'll keep learning more approaches as the course progresses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knyuUv9cqsoY"
      },
      "source": [
        "### Talkin' like Yoda\n",
        "Languages like English are built around the _subject-verb-object_ pattern. But if you're familiar with Yoda from Star Wars, he famously speaks in an _object-subject-verb pattern_. Using the information in a dependency parse, we can turn basic English sentences into Yoda-speak."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9AydbEIqsRQ"
      },
      "outputs": [],
      "source": [
        "def yodize(s: str):\n",
        "  doc = nlp(s)\n",
        "  for t in doc:\n",
        "    if t.dep_ == \"ROOT\":\n",
        "\n",
        "      # Assuming our sentence is of the form subject-verb-object, we take\n",
        "      # everything after the root (likely verb) and put it in front, and\n",
        "      # likewise take everything before the root, and put it after.\n",
        "      seq = [doc[t.i + 1: -1].text, doc[0: t.i].text, t.text + '.']\n",
        "      seq[0] = seq[0].capitalize()\n",
        "      print(' '.join(seq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIa8Cziwqqnf"
      },
      "outputs": [],
      "source": [
        "yodize(\"I will fly to Texas.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofEnieaJZ8eX"
      },
      "source": [
        "This is ok for simple sentences but starts getting weird with longer, more convoluted sentences. What are some ways you would improve this?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V92TUxWioNtq"
      },
      "source": [
        "### Advanced Preprocessing Exercises"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ltil7XSyzMe"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# EXERCISE: Learn how to extend spaCy's NER models. Specifically, how to add new\n",
        "# entity names and entity types.\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1P58pxYkoN0j"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# EXERCISE: using doc.ents, identify and print the dates in this sentence.\n",
        "# Expected output: ['Feb 13th', 'Feb 24th']\n",
        "#\n",
        "s = \"We'll be in Osaka on Feb 13th and leave on Feb 24th.\"\n",
        "doc = nlp(s)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVFi0bxCoN4N"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# EXERCISE: Read about spaCy's PhraseMatcher\n",
        "# https://spacy.io/usage/rule-based-matching#phrasematcher\n",
        "#\n",
        "# Using the PhraseMatcher, find the start and end index of all occurrences\n",
        "# of 'Caesar Augustus' and 'Roman Empire' (case-insensitive).\n",
        "#\n",
        "# Expected output: [(0, 2), (15, 17)]\n",
        "#\n",
        "from spacy.matcher import PhraseMatcher\n",
        "s = \"Caesar Augustus was the founder of the Roman Principate (the first phase of the Roman Empire).\"\n",
        "doc = nlp(s)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nhN7p9G8taJ"
      },
      "source": [
        "# Additional Reading and Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM4G2KWa8wXO"
      },
      "source": [
        "Read through this page to learn more about spaCy's language processing pipeline including what's going on under the hood, how to create custom components, disable certain components (e.g. NER) when they're unneeded, optimization tips, and best practices:<br>\n",
        "https://spacy.io/usage/processing-pipelines\n",
        "<br><br>\n",
        "Take the free and succinct spaCy course (available in multiple languages):<br>\n",
        "https://course.spacy.io/\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "nlpdemystified-preprocessing.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
