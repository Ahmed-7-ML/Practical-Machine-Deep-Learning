{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39ea4834",
   "metadata": {},
   "source": [
    "## *Basic Terminologies:*\n",
    "1. *`Corpus(Blog) ` Large and Organized Collection of Text, This Text can be anything such as articles, books, social media posts or recorder conversations. It is Raw Data used to Train Language Models*\n",
    "\n",
    "2. *`Documnent ` Single Piece of Text within a Corpus. It can be a single sentence, paragraph or an article.*\n",
    "    - *Corpus Consist of Mltiple Documents.*\n",
    "\n",
    "3. *`Vocabulary(Dictionary) ` List of all the unique words that appear in the entire corpus, It doesnot contain duplicate words & it records each word only once.*\n",
    "\n",
    "4. *`Word ` simplest unit of text. This single word can occur multiple times in a document or blog. *\n",
    "    - *Each Document conists of a collection of Words.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732e482a",
   "metadata": {},
   "source": [
    "### `Anthor ways to convert a word into vector:`\n",
    "- `One-Hot Encoding `: \n",
    "The simplest way to represent words as vectors. The idea is to create a vector the size of the entire dictionary. For each word, this vector is all zeros except for one field that takes the value 1, which determines the word's position in the dictionary.\n",
    "\n",
    "Advantages:\n",
    " - Very simple and easy to understand and implement.\n",
    "\n",
    "Disadvantages:\n",
    " - `The Curse of Dimensionality`: If the dictionary is large (tens of thousands of words), the vector becomes very large and full of zeros, consuming significant memory and increasing processing time.\n",
    "\n",
    " - `Understandable`: Each vector is completely independent of the other. There is no relationship between the vector for \"king\" and the vector for \"queen,\" despite the similarity in meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b637213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word -> Cat\n",
      "Vocab -> {'Cat': 2, 'sat': 3, 'on': 4, 'the': 5, 'mat': 6}\n",
      "One-Hot Encoding -> [0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vocab = {'Cat':2, 'sat':3, 'on':4, 'the':5, 'mat':6}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "ohe_vector = np.zeros(vocab_size)\n",
    "\n",
    "word = 'Cat'\n",
    "ohe_vector[vocab[word]-1] = 1   \n",
    "\n",
    "print(\"Word ->\", word)\n",
    "print(\"Vocab ->\", vocab)\n",
    "print(\"One-Hot Encoding ->\", ohe_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c950fe46",
   "metadata": {},
   "source": [
    "### `N-Grams`:\n",
    "It's not a method for converting words to vectors like One-Hot, but rather a method for creating units of adjacent words. The idea is to take consecutive groups of n words.\n",
    "\n",
    "If n = 1, we call them Unigrams (single words).\n",
    "If n = 2, we call them Bigrams (pairs of words).\n",
    "If n = 3, we call them Trigrams (triads of words).\n",
    "\n",
    "This helps capture some context and meaning because it takes into account the order of words in a sentence.\n",
    "\n",
    "Advantages:\n",
    " - It takes into account word order: something neither Bag of Words nor One-Hot do.\n",
    " - Relatively simple: easy to understand and implement.\n",
    "\n",
    "Disadvantages:\n",
    " - It still suffers from the curse of dimensionality: each new N-gram added to the dictionary increases its size dramatically, especially if n is large.\n",
    " - The rare word problem: If a particular N-gram does not appear in the training data, its value will be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7211bd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'love'), ('love', 'natural'), ('natural', 'language'), ('language', 'processing')]\n",
      "[('I', 'love', 'natural'), ('love', 'natural', 'language'), ('natural', 'language', 'processing')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"I love natural language processing\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "print(bigrams)\n",
    "\n",
    "trigrams = list(ngrams(tokens, 3))\n",
    "print(trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e524b20",
   "metadata": {},
   "source": [
    "### *Summary :*\n",
    "1. `One Hot Encoding `: is a method for representing words as numerical vectors, \n",
    "2. `N-grams ` : are a method for representing words in context before transforming them into vectors (using One-Hot or other methods). \n",
    "\n",
    "*These methods are important fundamentals, but they are `not widely used in modern models based on Word Embeddings and Transformers` because they `cannot capture the meaning and complex relationships between words`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191ceee7",
   "metadata": {},
   "source": [
    "### Practical Implementation of all we talk about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a7216b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51245b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nMohamed Salah Hamed Mahrous Ghaly Egyptian Arabic pronunciation:\\nSalah began his senior career in 2010 at Al-Mokawloon, departing in 2012 to join Basel, where he won two Swiss Super League titles. \\nIn 2014, he joined Chelsea for a reported fee of £11 million, but limited gametime led to successive loans to Fiorentina and Roma, who later signed him permanently for €15 million.\\nIn the 2016–17 season, Salah was a key figure in Roma's unsuccessful title bid, reaching double figures in both goals and assists.\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = \"\"\"\n",
    "Mohamed Salah Hamed Mahrous Ghaly Egyptian Arabic pronunciation:\n",
    "Salah began his senior career in 2010 at Al-Mokawloon, departing in 2012 to join Basel, where he won two Swiss Super League titles. \n",
    "In 2014, he joined Chelsea for a reported fee of £11 million, but limited gametime led to successive loans to Fiorentina and Roma, who later signed him permanently for €15 million.\n",
    "In the 2016–17 season, Salah was a key figure in Roma's unsuccessful title bid, reaching double figures in both goals and assists.\n",
    "\"\"\"\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20dfb251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences -> ['\\nMohamed Salah Hamed Mahrous Ghaly Egyptian Arabic pronunciation:\\nSalah began his senior career in 2010 at Al-Mokawloon, departing in 2012 to join Basel, where he won two Swiss Super League titles.', 'In 2014, he joined Chelsea for a reported fee of £11 million, but limited gametime led to successive loans to Fiorentina and Roma, who later signed him permanently for €15 million.', \"In the 2016–17 season, Salah was a key figure in Roma's unsuccessful title bid, reaching double figures in both goals and assists.\"]\n",
      "Number of sentences -> 3\n",
      "<class 'list'>\n",
      "Tokens -> ['Mohamed', 'Salah', 'Hamed', 'Mahrous', 'Ghaly', 'Egyptian', 'Arabic', 'pronunciation', ':', 'Salah', 'began', 'his', 'senior', 'career', 'in', '2010', 'at', 'Al-Mokawloon', ',', 'departing', 'in', '2012', 'to', 'join', 'Basel', ',', 'where', 'he', 'won', 'two', 'Swiss', 'Super', 'League', 'titles', '.', 'In', '2014', ',', 'he', 'joined', 'Chelsea', 'for', 'a', 'reported', 'fee', 'of', '£11', 'million', ',', 'but', 'limited', 'gametime', 'led', 'to', 'successive', 'loans', 'to', 'Fiorentina', 'and', 'Roma', ',', 'who', 'later', 'signed', 'him', 'permanently', 'for', '€15', 'million', '.', 'In', 'the', '2016–17', 'season', ',', 'Salah', 'was', 'a', 'key', 'figure', 'in', 'Roma', \"'s\", 'unsuccessful', 'title', 'bid', ',', 'reaching', 'double', 'figures', 'in', 'both', 'goals', 'and', 'assists', '.']\n",
      "Number of tokens -> 96\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Tokenization (Word and Sentences)\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "sentences = sent_tokenize(corpus)\n",
    "tokens = word_tokenize(corpus)\n",
    "\n",
    "print(\"Sentences ->\", sentences)\n",
    "print(\"Number of sentences ->\", len(sentences))\n",
    "print(type(sentences))\n",
    "\n",
    "print(\"Tokens ->\", tokens)\n",
    "print(\"Number of tokens ->\", len(tokens))\n",
    "print(type(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe94c851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords -> {\"mightn't\", \"she's\", 'i', 'if', 'being', 'hadn', \"shan't\", \"i'd\", 'mustn', 'for', 'on', 'weren', 'out', \"you've\", 'into', 'of', 'once', 'where', 'over', 'this', 'his', 'through', 'when', 'he', 'most', 's', 'with', \"it'd\", \"hasn't\", 'were', 'she', 'whom', 'had', 'their', 'to', 'off', 'my', 'is', 'ours', 'yours', 'during', 'aren', 'hers', 'up', 'after', \"wasn't\", 'nor', 'here', 'and', 'me', \"i'll\", \"she'll\", \"wouldn't\", \"couldn't\", \"they'd\", 'no', 'more', 'all', 're', 'ourselves', \"it's\", 'wouldn', 'was', 'have', 'how', 'been', 'now', 'down', 'o', 'your', 'them', 'any', 'who', 'why', \"won't\", \"he'll\", 'yourself', 'doing', 'above', 'will', 'such', 'myself', 'the', 'both', \"i've\", 'same', 'wasn', 'do', \"mustn't\", \"doesn't\", 'which', 'our', 'her', \"hadn't\", 'itself', 'an', 'between', 'a', 'am', 'should', 'not', \"that'll\", 'those', 'these', \"they've\", \"weren't\", \"i'm\", 'but', 'against', 'other', \"we'll\", 'because', \"aren't\", 'before', \"haven't\", 'than', \"they're\", \"didn't\", 'about', 'very', 'as', \"she'd\", 'doesn', 'shouldn', 'only', 'at', 'further', 'shan', 'mightn', 'too', 'yourselves', 'just', 'm', 'some', 'what', \"you'd\", 'you', 'him', \"he's\", 've', 'd', 'did', 'having', 'we', 'don', 'didn', 'so', \"we'd\", 'ain', \"don't\", \"it'll\", 'that', 'themselves', 'couldn', \"should've\", \"they'll\", 'has', 'it', 'below', 'can', 'until', 'haven', \"he'd\", 'each', 'under', 'or', 'its', \"shouldn't\", 'does', 'theirs', 'they', \"we're\", 't', 'won', 'in', \"isn't\", 'are', 'hasn', 'ma', 'needn', 'again', 'y', \"you're\", 'by', \"we've\", \"you'll\", 'himself', 'isn', 'from', 'be', 'while', 'there', 'own', 'll', 'few', 'herself', \"needn't\", 'then'}\n",
      "Number of Stopwords -> 198\n",
      "<class 'set'>\n",
      "Cleaned Tokens -> ['Mohamed', 'Salah', 'Hamed', 'Mahrous', 'Ghaly', 'Egyptian', 'Arabic', 'pronunciation', ':', 'Salah', 'began', 'senior', 'career', '2010', 'Al-Mokawloon', ',', 'departing', '2012', 'join', 'Basel', ',', 'two', 'Swiss', 'Super', 'League', 'titles', '.', '2014', ',', 'joined', 'Chelsea', 'reported', 'fee', '£11', 'million', ',', 'limited', 'gametime', 'led', 'successive', 'loans', 'Fiorentina', 'Roma', ',', 'later', 'signed', 'permanently', '€15', 'million', '.', '2016–17', 'season', ',', 'Salah', 'key', 'figure', 'Roma', \"'s\", 'unsuccessful', 'title', 'bid', ',', 'reaching', 'double', 'figures', 'goals', 'assists', '.']\n",
      "Number of Cleaned Tokens -> 68\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Stopword Removal\n",
    "\n",
    "stop_words_english = set(stopwords.words('english'))\n",
    "print(\"Stopwords ->\", stop_words_english)\n",
    "print(\"Number of Stopwords ->\", len(stop_words_english))\n",
    "print(type(stop_words_english))\n",
    "\n",
    "cleaned_tokens = [t for t in tokens if t.lower() not in stop_words_english] \n",
    "print(\"Cleaned Tokens ->\", cleaned_tokens)\n",
    "print(\"Number of Cleaned Tokens ->\", len(cleaned_tokens))\n",
    "print(type(cleaned_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0eb2708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Words -> ['moham', 'salah', 'hame', 'mahrou', 'ghali', 'egyptian', 'arab', 'pronunci', ':', 'salah', 'began', 'senior', 'career', '2010', 'al-mokawloon', ',', 'depart', '2012', 'join', 'basel', ',', 'two', 'swiss', 'super', 'leagu', 'titl', '.', '2014', ',', 'join', 'chelsea', 'report', 'fee', '£11', 'million', ',', 'limit', 'gametim', 'led', 'success', 'loan', 'fiorentina', 'roma', ',', 'later', 'sign', 'perman', '€15', 'million', '.', '2016–17', 'season', ',', 'salah', 'key', 'figur', 'roma', \"'s\", 'unsuccess', 'titl', 'bid', ',', 'reach', 'doubl', 'figur', 'goal', 'assist', '.']\n",
      "Number of Stemmed Words -> 68\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stemmed_words = [stemmer.stem(word) for word in cleaned_tokens]\n",
    "print(\"Stemmed Words ->\", stemmed_words)\n",
    "print(\"Number of Stemmed Words ->\", len(stemmed_words))\n",
    "print(type(stemmed_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words -> ['Mohamed', 'Salah', 'Hamed', 'Mahrous', 'Ghaly', 'Egyptian', 'Arabic', 'pronunciation', ':', 'Salah', 'began', 'senior', 'career', '2010', 'Al-Mokawloon', ',', 'departing', '2012', 'join', 'Basel', ',', 'two', 'Swiss', 'Super', 'League', 'title', '.', '2014', ',', 'joined', 'Chelsea', 'reported', 'fee', '£11', 'million', ',', 'limited', 'gametime', 'led', 'successive', 'loan', 'Fiorentina', 'Roma', ',', 'later', 'signed', 'permanently', '€15', 'million', '.', '2016–17', 'season', ',', 'Salah', 'key', 'figure', 'Roma', \"'s\", 'unsuccessful', 'title', 'bid', ',', 'reaching', 'double', 'figure', 'goal', 'assist', '.']\n",
      "Number of Lemmatized Words -> 68\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in cleaned_tokens]\n",
    "print(\"Lemmatized Words ->\", lemmatized_words)\n",
    "print(\"Number of Lemmatized Words ->\", len(lemmatized_words))\n",
    "print(type(lemmatized_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c80704a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' mohamed salah hamed mahrous ghaly egyptian arabic pronunciation  salah began his senior career in 2010 at al mokawloon  departing in 2012 to join basel  where he won two swiss super league titles ',\n",
       " 'in 2014  he joined chelsea for a reported fee of  11 million  but limited gametime led to successive loans to fiorentina and roma  who later signed him permanently for  15 million ',\n",
       " 'in the 2016 17 season  salah was a key figure in roma s unsuccessful title bid  reaching double figures in both goals and assists ']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub(r'[^a-zA-Z0-9]', ' ', sentences[i])\n",
    "    review = review.lower()\n",
    "    corpus.append(review)\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a7843d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words -> [[0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 2 1\n",
      "  0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 2 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0 1]\n",
      " [1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 2 1 0 0 0 1 1 0 1 0\n",
      "  1 0 1 0 1 1 1 0 2 0 0 1 1 0 0 1 1 0 0 0 1 1 0 0 0 0 0 2 0 0 0 0 1 0]\n",
      " [0 0 1 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 3 0\n",
      "  0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 0]]\n",
      "Shape of Bag of Words -> (3, 70)\n"
     ]
    }
   ],
   "source": [
    "# Bag of Words\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(corpus).toarray()\n",
    "print(\"Bag of Words ->\", X)\n",
    "print(\"Shape of Bag of Words ->\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['11' '15' '17' '2010' '2012' '2014' '2016' 'al' 'and' 'arabic' 'assists'\n",
      " 'at' 'basel' 'began' 'bid' 'both' 'but' 'career' 'chelsea' 'departing'\n",
      " 'double' 'egyptian' 'fee' 'figure' 'figures' 'fiorentina' 'for'\n",
      " 'gametime' 'ghaly' 'goals' 'hamed' 'he' 'him' 'his' 'in' 'join' 'joined'\n",
      " 'key' 'later' 'league' 'led' 'limited' 'loans' 'mahrous' 'million'\n",
      " 'mohamed' 'mokawloon' 'of' 'permanently' 'pronunciation' 'reaching'\n",
      " 'reported' 'roma' 'salah' 'season' 'senior' 'signed' 'successive' 'super'\n",
      " 'swiss' 'the' 'title' 'titles' 'to' 'two' 'unsuccessful' 'was' 'where'\n",
      " 'who' 'won']\n",
      "Length of Vocabulary -> 70\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out())\n",
    "print(\"Length of Vocabulary ->\", len(vectorizer.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0c1a391a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mohamed': 45, 'salah': 53, 'hamed': 30, 'mahrous': 43, 'ghaly': 28, 'egyptian': 21, 'arabic': 9, 'pronunciation': 49, 'began': 13, 'his': 33, 'senior': 55, 'career': 17, 'in': 34, '2010': 3, 'at': 11, 'al': 7, 'mokawloon': 46, 'departing': 19, '2012': 4, 'to': 63, 'join': 35, 'basel': 12, 'where': 67, 'he': 31, 'won': 69, 'two': 64, 'swiss': 59, 'super': 58, 'league': 39, 'titles': 62, '2014': 5, 'joined': 36, 'chelsea': 18, 'for': 26, 'reported': 51, 'fee': 22, 'of': 47, '11': 0, 'million': 44, 'but': 16, 'limited': 41, 'gametime': 27, 'led': 40, 'successive': 57, 'loans': 42, 'fiorentina': 25, 'and': 8, 'roma': 52, 'who': 68, 'later': 38, 'signed': 56, 'him': 32, 'permanently': 48, '15': 1, 'the': 60, '2016': 6, '17': 2, 'season': 54, 'was': 66, 'key': 37, 'figure': 23, 'unsuccessful': 65, 'title': 61, 'bid': 14, 'reaching': 50, 'double': 20, 'figures': 24, 'both': 15, 'goals': 29, 'assists': 10}\n",
      "Length of Vocabulary Dictionary -> 70\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)\n",
    "print(\"Length of Vocabulary Dictionary ->\", len(vectorizer.vocabulary_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diabetes_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
