{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce22711d",
   "metadata": {},
   "source": [
    "### بسم الله الرحمن الرحيم\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1ac6ed",
   "metadata": {},
   "source": [
    "## Day1- Natural Language Processing\n",
    "\n",
    "- Tokenization\n",
    "- Stemming\n",
    "- Lemmtiazation\n",
    "- Bag-of-Words\n",
    "\n",
    "### Text Preprocessing Techniques:\n",
    "\n",
    "Tokenization -> StopWords -> Stemming -> Lemmtization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cb26dd",
   "metadata": {},
   "source": [
    "### 1. _`Tokenization`_\n",
    "\n",
    "- Theory: This is the process of dividing text into smaller parts called tokens, which can be words, symbols, or even letters.\n",
    "- The goal is to convert the raw text into an organized list that the program can process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e1f093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'how', 'are', 'you', 'doing', 'today', '?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Split Text into Words\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"Hello, how are you doing today?\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464cb346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, how are you doing today?', 'The weather is great, and Python is awesome.', 'The sky is pinkish-blue.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "# Split Text into Sentences\n",
    "\n",
    "text = \"Hello, how are you doing today? The weather is great, and Python is awesome. The sky is pinkish-blue.\"\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3746a57c",
   "metadata": {},
   "source": [
    "### 2. _`StopWords Removal`_\n",
    "\n",
    "- Theory: Words that are very common in any language, but usually don't add any substantive meaning to the text, such as \"in,\" \"of,\" \"to,\" \"on,\" \"this,\" and \"or.\"\n",
    "- In many natural language processing applications such as text classification, these words are ignored because their presence in a text doesn't affect its underlying meaning, and removing them reduces data size and speeds up processing.\n",
    "- This includes converting all letters to lowercase, removing punctuation, and removing meaningless words like \"from\" and \"to\" (called stop words).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5846e962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'today']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Download the stopwords from NLTK\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# List of stop words in English\n",
    "stop_words_english = set(stopwords.words('english'))\n",
    "# List of punctuation characters\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "cleaned_tokens = []\n",
    "for token in tokens:\n",
    "    # Convert to lowercase & Remove punctuation & Remove stop words\n",
    "    clean_token = token.lower().strip(string.punctuation)\n",
    "\n",
    "    if clean_token and clean_token not in stop_words_english:\n",
    "        cleaned_tokens.append(clean_token)\n",
    "\n",
    "print(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a127d0aa",
   "metadata": {},
   "source": [
    "### 3. _`Stemming`_ is the process of finding the \"root\" of a word, or the common part shared by a group of words.\n",
    "\n",
    "- The goal is to convert words that are similar in meaning but different in form into a single root.\n",
    "- This process is often `quick`, but it doesn't respect grammatical rules and may result in roots that are not real or not found in the dictionary. (Maybe Meaningless)\n",
    "\n",
    "Example:\n",
    "Words: \"go\", \"went\", \"went\", \"goes\", \"going\"\n",
    "Stem: \"go\"\n",
    "\n",
    "Another example that might be wrong:\n",
    "Words: \"houses,\" \"house.\"\n",
    "Stem: \"house\" (correct)\n",
    "\n",
    "However, the word \"hospital\" might have a root of \"she will heal,\" which would be meaningless.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dd6cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go', 'went', 'gone', 'goe', 'go', 'histori', 'histor', 'histor']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "words = [\"go\", \"went\", \"gone\", \"goes\", \"going\",\n",
    "         'history', 'historical', 'historically']\n",
    "stemmed_words = [ps.stem(w) for w in words]\n",
    "\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd78fef8",
   "metadata": {},
   "source": [
    "### 4. _`Lemmatization`_ is a more precise process than stemming. It aims to find the base form of a word (the lemma) that is a real word found in the dictionary. This process takes into account grammatical rules (such as verb conjugation or noun case) to ensure that the base form makes sense , but slower than stemming.\n",
    "\n",
    "Example:\n",
    "The words: \"they go,\" \"went,\" \"she went.\"\n",
    "Lemma: \"went\" (past tense)\n",
    "\n",
    "Another example:\n",
    "The words: \"ate,\" \"I eat.\"\n",
    "Lemma: \"ate\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd008426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go', 'go', 'go', 'go', 'go', 'better', 'run', 'fairly']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download the WordNet data from NLTK\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [\"go\", \"went\", \"gone\", \"goes\", \"going\", 'better', 'running', 'fairly']\n",
    "\n",
    "lemmatized_words = [lemmatizer.lemmatize(w, pos='v') for w in words]\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5539815",
   "metadata": {},
   "source": [
    "_`Stemming`: Faster, doesn't respect grammar rules, and may produce nonexistent roots, `Lemmatization`: Slower, more accurate, and ensures that the base form is a real word and exists in the dictionary._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d602b9",
   "metadata": {},
   "source": [
    "### 4. _`Bag-of-Words (BoW)`_ : It is a Feature Extraction Technique method of `converting text into a digital representation (vector)` by counting the number of times each word occurs in the text.\n",
    "\n",
    "- In the Bag of Words model, word order and grammar (sentence, grammar, etc.) are ignored, and the focus is solely on the frequency of each word. Imagine taking all the words from a sentence or document and putting them in a bag. All that matters is the number of times each word appears, not its order.\n",
    "\n",
    "Example:\n",
    "Sentence 1: \"The cat eats the fish.\"\n",
    "\n",
    "Sentence 2: \"The fish is eaten by the cat.\"\n",
    "\n",
    "In the Bag of Words model, both sentences are represented the same way because the same words (cat, eat, fish) are present in the same number in both sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "026ebeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['by' 'cat' 'eaten' 'eats' 'fish' 'is' 'the']\n",
      "Vectors:\n",
      " [[0 1 0 1 1 0 2]\n",
      " [1 1 1 0 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The process of creating a Bag of Words model consists of two basic steps:\n",
    "1. Creating a Vocabulary: \n",
    "    We collect all the unique words from all the texts we have. \n",
    "    This dictionary is the basis of our model.\n",
    "\n",
    "2. Vectorizing the Text: \n",
    "    For each text, we create a vector equal to the size of the dictionary.\n",
    "    Each cell in this vector represents a word from the dictionary, and its value is the number of times that word occurs in the text.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"The cat eats the fish.\",\n",
    "    \"The fish is eaten by the cat.\"\n",
    "]\n",
    "\n",
    "# Create the Bag of Words model\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Convert Text into Vectors\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Display the vocabulary (Unique Words)\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the vectors\n",
    "print(\"Vectors:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ceee86",
   "metadata": {},
   "source": [
    "_`Disadvantages: `_\n",
    "\n",
    "- `Ignores word order`: This can lead to loss of meaning.\n",
    "\n",
    "- `It produces large vectors`: The larger the dictionary, the larger the vector's dimensions, causing the \"curse of dimensionality.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fabe8c0",
   "metadata": {},
   "source": [
    "### 5. _`Converting Words to Numbers (Word Embeddings)`_\n",
    "\n",
    "- Theory: Computers don't understand words, they understand numbers.\n",
    "- Word Embeddings is a method of converting each word into a digital vector in a multidimensional space.\n",
    "- Words with similar meanings have their vectors close to each other in this space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a7fa4e",
   "metadata": {},
   "source": [
    "_Now, we use the Gensim library to create a Word2Vec model, which is one of the most popular embedding methods._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'hello': [ 7.0887972e-03 -1.5679300e-03  7.9474989e-03 -9.4886590e-03\n",
      " -8.0294991e-03 -6.6403709e-03 -4.0034545e-03  4.9892161e-03\n",
      " -3.8135587e-03 -8.3199050e-03  8.4117772e-03 -3.7470020e-03\n",
      "  8.6086961e-03 -4.8957514e-03  3.9185942e-03  4.9220170e-03\n",
      "  2.3926091e-03 -2.8188038e-03  2.8491246e-03 -8.2562361e-03\n",
      " -2.7655398e-03 -2.5911583e-03  7.2490061e-03 -3.4634031e-03\n",
      " -6.5997029e-03  4.3404270e-03 -4.7448516e-04 -3.5975564e-03\n",
      "  6.8824720e-03  3.8723124e-03 -3.9002013e-03  7.7188847e-04\n",
      "  9.1435025e-03  7.7546560e-03  6.3618720e-03  4.6673026e-03\n",
      "  2.3844899e-03 -1.8416261e-03 -6.3712932e-03 -3.0181051e-04\n",
      " -1.5653884e-03 -5.7228567e-04 -6.2628710e-03  7.4340473e-03\n",
      " -6.5914928e-03 -7.2392775e-03 -2.7571463e-03 -1.5154004e-03\n",
      " -7.6357173e-03  6.9824100e-04 -5.3261113e-03 -1.2755442e-03\n",
      " -7.3651113e-03  1.9605684e-03  3.2731986e-03 -2.3138524e-05\n",
      " -5.4483581e-03 -1.7260861e-03  7.0849168e-03  3.7362587e-03\n",
      " -8.8810492e-03 -3.4135508e-03  2.3541022e-03  2.1380198e-03\n",
      " -9.4640078e-03  4.5711659e-03 -8.6569972e-03 -7.3870681e-03\n",
      "  3.4831120e-03 -3.4709584e-03  3.5644709e-03  8.8940905e-03\n",
      " -3.5743224e-03  9.3204249e-03  1.7110384e-03  9.8477742e-03\n",
      "  5.7050432e-03 -9.1494834e-03 -3.3277308e-03  6.5301750e-03\n",
      "  5.6027793e-03  8.7055154e-03  6.9261026e-03  8.0388878e-03\n",
      " -9.8230084e-03  4.2988253e-03 -5.0300765e-03  3.5123860e-03\n",
      "  6.0566878e-03  4.3921317e-03  7.5123594e-03  1.4977157e-03\n",
      " -1.2649416e-03  5.7684006e-03 -5.6395675e-03  3.8591625e-05\n",
      "  9.4565870e-03 -5.4812501e-03  3.8142789e-03 -8.1130210e-03]\n",
      "Most similar words to 'hello':\n",
      "[('about', 0.1088901236653328), ('you', 0.06285077333450317), ('hope', 0.05048205703496933), ('well', 0.02680680900812149), ('great', 0.020000366494059563), ('am', 0.015025208704173565), ('are', 0.01297997497022152), ('I', -0.1019841730594635), ('doing', -0.13429947197437286), ('how', -0.25888726115226746)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\n",
    "    [\"hello\", \"how\", \"are\", \"you\"],\n",
    "    [\"I\", \"am\", \"doing\", \"well\"],\n",
    "    [\"how\", \"about\", \"you\"],\n",
    "    [\"I\", \"hope\", \"you\", \"are\", \"doing\", \"great\"]\n",
    "]\n",
    "\n",
    "# Model training\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Accessing the vector for the word 'hello'\n",
    "vector_hello = model.wv['hello']\n",
    "\n",
    "# Get similar words to 'hello'\n",
    "print(\"Vector for 'hello':\", vector_hello)\n",
    "print(\"Most similar words to 'hello':\")\n",
    "print(model.wv.most_similar('hello'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd58ad35",
   "metadata": {},
   "source": [
    "### 6. _`A Simple Practical Application: Sentiment Classification`_\n",
    "\n",
    "- Theory: We will classify a simple sentence as \"positive\" or \"negative.\"\n",
    "- This type of task is the basis of many natural language processing applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc24cd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text ->  I am very happy with this product\n",
      "Predicted Sentiment ->  positive\n",
      "Text ->  This is the worst experience I've had\n",
      "Predicted Sentiment ->  negative\n",
      "Model Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Bag-of-Words Example\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Naive Bayes Classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Sample data\n",
    "documents = [\n",
    "    (\"I love this product\", \"positive\"),\n",
    "    (\"This is the best purchase I've made\", \"positive\"),\n",
    "    (\"I'm very satisfied with my experience\", \"positive\"),\n",
    "    (\"This is a terrible product\", \"negative\"),\n",
    "    (\"I hate this item\", \"negative\"),\n",
    "    (\"This is the worst purchase I've made\", \"negative\")\n",
    "]\n",
    "\n",
    "# Split data into texts and labels\n",
    "texts = [d[0] for d in documents]\n",
    "labels = [d[1] for d in documents]\n",
    "\n",
    "# Pipeline -> (Vectorizer + Classifier)\n",
    "# CountVectorizer -> Convert Text into Bag-of-Words\n",
    "# MultinominalNB -> Naive Bayes Classifier\n",
    "model_pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "model_pipeline.fit(texts, labels)\n",
    "\n",
    "# Test the model\n",
    "test_sentences = [\n",
    "    \"I am very happy with this product\",\n",
    "    \"This is the worst experience I've had\"\n",
    "]\n",
    "\n",
    "predictions = model_pipeline.predict(test_sentences)\n",
    "\n",
    "print(\"Text -> \", test_sentences[0])\n",
    "print(\"Predicted Sentiment -> \", predictions[0])\n",
    "\n",
    "print(\"Text -> \", test_sentences[1])\n",
    "print(\"Predicted Sentiment -> \", predictions[1])\n",
    "\n",
    "# Accuracy Check\n",
    "test_labels = [\"positive\", \"negative\"]\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(\"Model Accuracy: \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diabetes_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
